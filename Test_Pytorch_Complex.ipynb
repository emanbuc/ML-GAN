{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_Pytorch-Complex.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNiz+ZuFRbYXWi6J/fTPQBF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emanbuc/ML-GAN/blob/main/Test_Pytorch_Complex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvcdrjVLWgE7",
        "outputId": "dce6d4db-9f3f-4218-deff-00145cf5da16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.2)\n",
            "Collecting torch\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 9.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.2\n",
            "    Uninstalling torch-1.10.2:\n",
            "      Successfully uninstalled torch-1.10.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.3 requires torch==1.10.2, but you have torch 1.11.0 which is incompatible.\n",
            "fastai 2.5.3 requires torch<1.11,>=1.7.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0\n",
            "Requirement already satisfied: pytorch-complex in /usr/local/lib/python3.7/dist-packages (0.0.8)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from pytorch-complex) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->pytorch-complex) (4.2.0)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --upgrade\n",
        "!pip install pytorch-complex\n",
        "!pip install fastai==2.5.3 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import  torchcomplex.nn as nnc\n",
        "import torch\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "copiIj5YW5AH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This discriminator is an MLP neural network \n",
        "class Discriminator(nnc.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nnc.Sequential(\n",
        "            #The input is two-dimensional, and the first hidden layer is composed \n",
        "            # of 256 neurons with ReLU activation.\n",
        "            nnc.Linear(2, 256),\n",
        "            nnc.CReLU(),\n",
        "            # use dropout after hidden layer to avoid overfitting.\n",
        "            nnc.Dropout(0.3),\n",
        "            nnc.Linear(256, 128),\n",
        "            nnc.CReLU(),\n",
        "            nnc.Dropout(0.3),\n",
        "            nnc.Linear(128, 64),\n",
        "            nnc.CReLU(),\n",
        "            nnc.Dropout(0.3),\n",
        "            nnc.Linear(64, 1),\n",
        "            # The output is composed of a single neuron with sigmoidal activation\n",
        "            # to represent a probability.\n",
        "            nnc.Sigmoid(),\n",
        "        )\n",
        "\n",
        "     # .forward() to describe how the output of the model is calculated\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "# instantiate a Discriminator object\n",
        "discriminator = Discriminator()"
      ],
      "metadata": {
        "id": "VU0xri7QXSSH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nnc.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nnc.Sequential(\n",
        "            nnc.Linear(2, 16),\n",
        "            nnc.CReLU(),\n",
        "            nnc.Linear(16, 32),\n",
        "            nnc.CReLU(),\n",
        "            nnc.Linear(32, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "generator = Generator()"
      ],
      "metadata": {
        "id": "EeyPZdGoZ9F1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "num_epochs = 300\n",
        "\n",
        "# The binary cross-entropy function is a suitable loss function for training the discriminator \n",
        "# because it considers a binary classification task. \n",
        "# It’s also suitable for training the generator since it feeds its output to the discriminator,\n",
        "# which provides a binary observable output.\n",
        "#loss_function = torch.nn.BCELoss()\n",
        "# ne serve una che support ai numeri complessi\n",
        "loss_function = torch.nn.L1Loss()"
      ],
      "metadata": {
        "id": "3qkawDJnai_X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "JFK49AZvamBM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Training Data from EIS Dataset"
      ],
      "metadata": {
        "id": "W_4LyFCya4fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from fastai.vision.all import *\n",
        "import sys"
      ],
      "metadata": {
        "id": "ByBhkLHqa7kU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load EB_ML python libraries\n",
        "# The following libraries are used in this notebook and should be installed in your local machine before running this notebook.\n",
        "# eb_colab_utils.py\n",
        "# eb_ml_battery_lib.py\n",
        "# eb_ml_utils.py\n",
        "\n",
        "# path to load external *.py files used in this notebook\n",
        "# Note: in Google Colab virtual machine you shoud copy the files in \"/content\" folder after BEFORE running this notebook's cell\n",
        "external_python_file_path=\"'/.'\"\n",
        "sys.path.append(external_python_file_path)\n",
        "\n",
        "\n",
        "from eb_ml_colab_utils import get_root_path,copy_model_to_google_drive\n",
        "from eb_ml_battery_lib import load_soc_dataset,generate_image_files_from_eis\n",
        "from eb_ml_utils import save_model_weights,build_data_loader,build_and_train_learner,score_model"
      ],
      "metadata": {
        "id": "HblKeAioa9w8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#configuration dictionary\n",
        "config ={}\n",
        "\n",
        "# Root working folder (local or Google Drive)\n",
        "# config['ROOT_DIR'] = get_root_path(\"batterie\")\n",
        "config['ROOT_DIR'] = get_root_path(\"batterie\")  \n",
        "\n",
        "# Folder with dataset in CSV format\n",
        "#config['DATASETS_DIR'] = config['ROOT_DIR']+\"/datasets\"\n",
        "config['DATASETS_DIR'] = config['ROOT_DIR']+\"/datasets/EIS-vs-SOC-2022\"\n",
        "\n",
        "# List of SoC level into dataset\n",
        "#config['soc_list']=['100','090','080','070','060','050','040','030','020','010']\n",
        "config['soc_list']=['100','090','080','070','060','050','040','030','020','010']\n",
        "\n",
        "\n",
        "# Folder to store trained model\n",
        "#config['MODELS_DIR'] = config['ROOT_DIR']+\"/models\"\n",
        "config['MODELS_DIR'] = config['ROOT_DIR']+\"/models\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq1sfYt-bF-l",
        "outputId": "2e5f530a-3fce-46ca-ea73-7fcaacee03b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on COLAB\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data acquition file to load from dateset folder\n",
        "# EIS Dataset https://data.mendeley.com/datasets/ch3sydbbrg/2\n",
        "frequency_list=[ 0.05, 0.1, 0.2, 0.4, 1, 2, 4, 10, 20, 40, 100, 200, 400, 1000]\n",
        "battery_list=[1,2,3,4,5,7,8,9,10,11,12] # Data acquitions 6,13 to be used for TEST]\n",
        "dataset,feature_col_names=load_soc_dataset(battery_list,config[\"soc_list\"],config['DATASETS_DIR'])"
      ],
      "metadata": {
        "id": "rCde4RBybIwl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_col_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJYjcl-GbLLT",
        "outputId": "36e618d4-e531-4d0b-b71d-b566466d7269"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Z_f0',\n",
              " 'Z_f1',\n",
              " 'Z_f2',\n",
              " 'Z_f3',\n",
              " 'Z_f4',\n",
              " 'Z_f5',\n",
              " 'Z_f6',\n",
              " 'Z_f7',\n",
              " 'Z_f8',\n",
              " 'Z_f9',\n",
              " 'Z_f10',\n",
              " 'Z_f11',\n",
              " 'Z_f12',\n",
              " 'Z_f13']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soc_10_training=dataset.query('SOC== \"010\"')[feature_col_names]\n",
        "soc_10_training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "s_xv0MZBbOQc",
        "outputId": "c3a52a93-8f92-467c-f65f-4317f7618e98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Z_f0                Z_f1                Z_f2  \\\n",
              "010  0.079771-0.005349j  0.077740-0.004683j  0.076173-0.003523j   \n",
              "010  0.078894-0.004942j  0.077379-0.003965j  0.076076-0.003162j   \n",
              "010  0.074923-0.004870j  0.073477-0.003759j  0.072532-0.002710j   \n",
              "010  0.074411-0.004880j  0.072787-0.003883j  0.071541-0.003067j   \n",
              "010  0.078197-0.004944j  0.076708-0.003921j  0.075516-0.002831j   \n",
              "010  0.073499-0.004352j  0.071975-0.003375j  0.071127-0.002276j   \n",
              "010  0.075002-0.004450j  0.073594-0.003316j  0.072728-0.002386j   \n",
              "010  0.079897-0.004582j  0.078391-0.003490j  0.077319-0.002565j   \n",
              "010  0.077036-0.004560j  0.075534-0.003552j  0.074379-0.002565j   \n",
              "010  0.097612-0.004647j  0.095967-0.003856j  0.094797-0.002671j   \n",
              "010  0.071875-0.003984j  0.070516-0.003260j  0.069583-0.002225j   \n",
              "\n",
              "                   Z_f3                Z_f4                Z_f5  \\\n",
              "010  0.075399-0.003229j  0.073304-0.003548j  0.071919-0.004337j   \n",
              "010  0.075439-0.002577j  0.073901-0.002919j  0.072642-0.003744j   \n",
              "010  0.071758-0.002316j  0.070510-0.002377j  0.069565-0.003037j   \n",
              "010  0.070933-0.002429j  0.069589-0.002628j  0.068560-0.003292j   \n",
              "010  0.074972-0.002334j  0.073589-0.002613j  0.072889-0.003227j   \n",
              "010  0.070579-0.001976j  0.069628-0.001913j  0.069061-0.002316j   \n",
              "010  0.072421-0.002074j  0.071293-0.002048j  0.070492-0.002500j   \n",
              "010  0.076870-0.001969j  0.075815-0.002046j  0.075152-0.002592j   \n",
              "010  0.073773-0.002133j  0.072797-0.002270j  0.072098-0.002704j   \n",
              "010  0.094201-0.002246j  0.093193-0.002260j  0.092317-0.002724j   \n",
              "010  0.069102-0.001781j  0.068355-0.001730j  0.067728-0.002101j   \n",
              "\n",
              "                   Z_f6                Z_f7                Z_f8  \\\n",
              "010  0.069978-0.004903j  0.066565-0.005511j  0.063821-0.005493j   \n",
              "010  0.070944-0.004596j  0.067612-0.005322j  0.064984-0.005585j   \n",
              "010  0.068364-0.003993j  0.065447-0.005366j  0.062517-0.006183j   \n",
              "010  0.067239-0.003997j  0.064362-0.005227j  0.061435-0.005945j   \n",
              "010  0.071461-0.004222j  0.068578-0.005554j  0.065428-0.006255j   \n",
              "010  0.068349-0.002973j  0.066075-0.004737j  0.063371-0.005850j   \n",
              "010  0.069626-0.003237j  0.067319-0.004893j  0.064682-0.005829j   \n",
              "010  0.074303-0.003259j  0.071812-0.004872j  0.069079-0.005790j   \n",
              "010  0.071250-0.003512j  0.068566-0.005268j  0.065698-0.006345j   \n",
              "010  0.091361-0.003504j  0.088793-0.004872j  0.086085-0.005751j   \n",
              "010  0.066984-0.002790j  0.065122-0.004238j  0.062783-0.005229j   \n",
              "\n",
              "                   Z_f9               Z_f10               Z_f11  \\\n",
              "010  0.061411-0.005366j  0.057958-0.005174j  0.055633-0.004818j   \n",
              "010  0.062448-0.005382j  0.059086-0.005205j  0.056763-0.004722j   \n",
              "010  0.059429-0.006285j  0.055555-0.005599j  0.053129-0.004991j   \n",
              "010  0.058558-0.005913j  0.054922-0.005389j  0.052485-0.004896j   \n",
              "010  0.062342-0.006436j  0.058299-0.005866j  0.055709-0.005264j   \n",
              "010  0.060204-0.006134j  0.056277-0.005871j  0.053678-0.004992j   \n",
              "010  0.061710-0.006009j  0.057768-0.005385j  0.055370-0.004826j   \n",
              "010  0.065966-0.006023j  0.062122-0.005708j  0.059560-0.004794j   \n",
              "010  0.062095-0.006770j  0.057652-0.006236j  0.054922-0.005423j   \n",
              "010  0.083177-0.006003j  0.079377-0.005685j  0.076842-0.005000j   \n",
              "010  0.059880-0.005821j  0.056042-0.005589j  0.053541-0.004915j   \n",
              "\n",
              "                  Z_f12               Z_f13  \n",
              "010  0.053745-0.004118j  0.051447-0.003001j  \n",
              "010  0.054563-0.004204j  0.052500-0.002999j  \n",
              "010  0.050902-0.004287j  0.048653-0.003007j  \n",
              "010  0.050414-0.004192j  0.048036-0.003069j  \n",
              "010  0.053496-0.004382j  0.051192-0.003051j  \n",
              "010  0.051553-0.004286j  0.049336-0.002925j  \n",
              "010  0.053423-0.003888j  0.051324-0.002779j  \n",
              "010  0.057592-0.003948j  0.055631-0.002843j  \n",
              "010  0.052739-0.004440j  0.050409-0.003261j  \n",
              "010  0.074681-0.004194j  0.072418-0.002935j  \n",
              "010  0.051409-0.004200j  0.049157-0.002987j  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d08fb6f3-00ac-4faf-9170-a7a1a03f900d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Z_f0</th>\n",
              "      <th>Z_f1</th>\n",
              "      <th>Z_f2</th>\n",
              "      <th>Z_f3</th>\n",
              "      <th>Z_f4</th>\n",
              "      <th>Z_f5</th>\n",
              "      <th>Z_f6</th>\n",
              "      <th>Z_f7</th>\n",
              "      <th>Z_f8</th>\n",
              "      <th>Z_f9</th>\n",
              "      <th>Z_f10</th>\n",
              "      <th>Z_f11</th>\n",
              "      <th>Z_f12</th>\n",
              "      <th>Z_f13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.079771-0.005349j</td>\n",
              "      <td>0.077740-0.004683j</td>\n",
              "      <td>0.076173-0.003523j</td>\n",
              "      <td>0.075399-0.003229j</td>\n",
              "      <td>0.073304-0.003548j</td>\n",
              "      <td>0.071919-0.004337j</td>\n",
              "      <td>0.069978-0.004903j</td>\n",
              "      <td>0.066565-0.005511j</td>\n",
              "      <td>0.063821-0.005493j</td>\n",
              "      <td>0.061411-0.005366j</td>\n",
              "      <td>0.057958-0.005174j</td>\n",
              "      <td>0.055633-0.004818j</td>\n",
              "      <td>0.053745-0.004118j</td>\n",
              "      <td>0.051447-0.003001j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.078894-0.004942j</td>\n",
              "      <td>0.077379-0.003965j</td>\n",
              "      <td>0.076076-0.003162j</td>\n",
              "      <td>0.075439-0.002577j</td>\n",
              "      <td>0.073901-0.002919j</td>\n",
              "      <td>0.072642-0.003744j</td>\n",
              "      <td>0.070944-0.004596j</td>\n",
              "      <td>0.067612-0.005322j</td>\n",
              "      <td>0.064984-0.005585j</td>\n",
              "      <td>0.062448-0.005382j</td>\n",
              "      <td>0.059086-0.005205j</td>\n",
              "      <td>0.056763-0.004722j</td>\n",
              "      <td>0.054563-0.004204j</td>\n",
              "      <td>0.052500-0.002999j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.074923-0.004870j</td>\n",
              "      <td>0.073477-0.003759j</td>\n",
              "      <td>0.072532-0.002710j</td>\n",
              "      <td>0.071758-0.002316j</td>\n",
              "      <td>0.070510-0.002377j</td>\n",
              "      <td>0.069565-0.003037j</td>\n",
              "      <td>0.068364-0.003993j</td>\n",
              "      <td>0.065447-0.005366j</td>\n",
              "      <td>0.062517-0.006183j</td>\n",
              "      <td>0.059429-0.006285j</td>\n",
              "      <td>0.055555-0.005599j</td>\n",
              "      <td>0.053129-0.004991j</td>\n",
              "      <td>0.050902-0.004287j</td>\n",
              "      <td>0.048653-0.003007j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.074411-0.004880j</td>\n",
              "      <td>0.072787-0.003883j</td>\n",
              "      <td>0.071541-0.003067j</td>\n",
              "      <td>0.070933-0.002429j</td>\n",
              "      <td>0.069589-0.002628j</td>\n",
              "      <td>0.068560-0.003292j</td>\n",
              "      <td>0.067239-0.003997j</td>\n",
              "      <td>0.064362-0.005227j</td>\n",
              "      <td>0.061435-0.005945j</td>\n",
              "      <td>0.058558-0.005913j</td>\n",
              "      <td>0.054922-0.005389j</td>\n",
              "      <td>0.052485-0.004896j</td>\n",
              "      <td>0.050414-0.004192j</td>\n",
              "      <td>0.048036-0.003069j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.078197-0.004944j</td>\n",
              "      <td>0.076708-0.003921j</td>\n",
              "      <td>0.075516-0.002831j</td>\n",
              "      <td>0.074972-0.002334j</td>\n",
              "      <td>0.073589-0.002613j</td>\n",
              "      <td>0.072889-0.003227j</td>\n",
              "      <td>0.071461-0.004222j</td>\n",
              "      <td>0.068578-0.005554j</td>\n",
              "      <td>0.065428-0.006255j</td>\n",
              "      <td>0.062342-0.006436j</td>\n",
              "      <td>0.058299-0.005866j</td>\n",
              "      <td>0.055709-0.005264j</td>\n",
              "      <td>0.053496-0.004382j</td>\n",
              "      <td>0.051192-0.003051j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.073499-0.004352j</td>\n",
              "      <td>0.071975-0.003375j</td>\n",
              "      <td>0.071127-0.002276j</td>\n",
              "      <td>0.070579-0.001976j</td>\n",
              "      <td>0.069628-0.001913j</td>\n",
              "      <td>0.069061-0.002316j</td>\n",
              "      <td>0.068349-0.002973j</td>\n",
              "      <td>0.066075-0.004737j</td>\n",
              "      <td>0.063371-0.005850j</td>\n",
              "      <td>0.060204-0.006134j</td>\n",
              "      <td>0.056277-0.005871j</td>\n",
              "      <td>0.053678-0.004992j</td>\n",
              "      <td>0.051553-0.004286j</td>\n",
              "      <td>0.049336-0.002925j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.075002-0.004450j</td>\n",
              "      <td>0.073594-0.003316j</td>\n",
              "      <td>0.072728-0.002386j</td>\n",
              "      <td>0.072421-0.002074j</td>\n",
              "      <td>0.071293-0.002048j</td>\n",
              "      <td>0.070492-0.002500j</td>\n",
              "      <td>0.069626-0.003237j</td>\n",
              "      <td>0.067319-0.004893j</td>\n",
              "      <td>0.064682-0.005829j</td>\n",
              "      <td>0.061710-0.006009j</td>\n",
              "      <td>0.057768-0.005385j</td>\n",
              "      <td>0.055370-0.004826j</td>\n",
              "      <td>0.053423-0.003888j</td>\n",
              "      <td>0.051324-0.002779j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.079897-0.004582j</td>\n",
              "      <td>0.078391-0.003490j</td>\n",
              "      <td>0.077319-0.002565j</td>\n",
              "      <td>0.076870-0.001969j</td>\n",
              "      <td>0.075815-0.002046j</td>\n",
              "      <td>0.075152-0.002592j</td>\n",
              "      <td>0.074303-0.003259j</td>\n",
              "      <td>0.071812-0.004872j</td>\n",
              "      <td>0.069079-0.005790j</td>\n",
              "      <td>0.065966-0.006023j</td>\n",
              "      <td>0.062122-0.005708j</td>\n",
              "      <td>0.059560-0.004794j</td>\n",
              "      <td>0.057592-0.003948j</td>\n",
              "      <td>0.055631-0.002843j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.077036-0.004560j</td>\n",
              "      <td>0.075534-0.003552j</td>\n",
              "      <td>0.074379-0.002565j</td>\n",
              "      <td>0.073773-0.002133j</td>\n",
              "      <td>0.072797-0.002270j</td>\n",
              "      <td>0.072098-0.002704j</td>\n",
              "      <td>0.071250-0.003512j</td>\n",
              "      <td>0.068566-0.005268j</td>\n",
              "      <td>0.065698-0.006345j</td>\n",
              "      <td>0.062095-0.006770j</td>\n",
              "      <td>0.057652-0.006236j</td>\n",
              "      <td>0.054922-0.005423j</td>\n",
              "      <td>0.052739-0.004440j</td>\n",
              "      <td>0.050409-0.003261j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.097612-0.004647j</td>\n",
              "      <td>0.095967-0.003856j</td>\n",
              "      <td>0.094797-0.002671j</td>\n",
              "      <td>0.094201-0.002246j</td>\n",
              "      <td>0.093193-0.002260j</td>\n",
              "      <td>0.092317-0.002724j</td>\n",
              "      <td>0.091361-0.003504j</td>\n",
              "      <td>0.088793-0.004872j</td>\n",
              "      <td>0.086085-0.005751j</td>\n",
              "      <td>0.083177-0.006003j</td>\n",
              "      <td>0.079377-0.005685j</td>\n",
              "      <td>0.076842-0.005000j</td>\n",
              "      <td>0.074681-0.004194j</td>\n",
              "      <td>0.072418-0.002935j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.071875-0.003984j</td>\n",
              "      <td>0.070516-0.003260j</td>\n",
              "      <td>0.069583-0.002225j</td>\n",
              "      <td>0.069102-0.001781j</td>\n",
              "      <td>0.068355-0.001730j</td>\n",
              "      <td>0.067728-0.002101j</td>\n",
              "      <td>0.066984-0.002790j</td>\n",
              "      <td>0.065122-0.004238j</td>\n",
              "      <td>0.062783-0.005229j</td>\n",
              "      <td>0.059880-0.005821j</td>\n",
              "      <td>0.056042-0.005589j</td>\n",
              "      <td>0.053541-0.004915j</td>\n",
              "      <td>0.051409-0.004200j</td>\n",
              "      <td>0.049157-0.002987j</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d08fb6f3-00ac-4faf-9170-a7a1a03f900d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d08fb6f3-00ac-4faf-9170-a7a1a03f900d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d08fb6f3-00ac-4faf-9170-a7a1a03f900d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_EIS_num = len(soc_10_training)\n",
        "n_frequency =len(frequency_list)\n",
        "train_data_length = total_EIS_num * n_frequency  #14*11\n",
        "\n",
        "# initialize train_data, a tensor with dimensions of <train_data_length> rows and 2 columns, all containing zeros. \n",
        "# Note: A tensor is a multidimensional array similar to a NumPy array.\n",
        "train_data = torch.zeros((train_data_length,2),dtype=torch.cfloat)"
      ],
      "metadata": {
        "id": "atINl46EbRMz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import mod\n",
        "\n",
        "for i  in range(0,train_data_length):\n",
        "  f_index = i % 14\n",
        "  row_index = i // 14\n",
        "  # the first column of train_data store the frequency values\n",
        "  train_data[i, 0] = frequency_list[f_index]\n",
        "  # the second column of the tensor  Z(f)\n",
        "  train_data[i, 1] = tensor(soc_10_training)[row_index,f_index]"
      ],
      "metadata": {
        "id": "wVI6eFM0bXh7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init tensor of labels, which are required by PyTorch’s data loader. \n",
        "# Since GANs make use of unsupervised learning techniques, the labels can be anything.\n",
        "train_labels = torch.zeros(train_data_length)\n",
        "\n",
        "# create train_set as a list of tuples, with each row of train_data and train_labels represented in each tuple \n",
        "# this is the format expected by PyTorch’s data loader.\n",
        "train_set = [\n",
        "    (train_data[i], train_labels[i]) for i in range(train_data_length)\n",
        "]"
      ],
      "metadata": {
        "id": "W4ob0w25bam0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.real(train_data[:, 1]), np.imag(train_data[:, 1]), \".\")"
      ],
      "metadata": {
        "id": "MgvrJK5bXBGl",
        "outputId": "c4c0057e-5751-4ce5-af5e-761be1580bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f36e2d7f6d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "WuCmtXaYkh2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch data loader called train_loader.\n",
        "# Data Loader will shuffle the data from train_set and return batches of <batch_size> samples \n",
        "# to be used to train the neural networks.\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True, drop_last=True\n",
        ")"
      ],
      "metadata": {
        "id": "6CyuEAJUbAwH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for n, (real_samples, _) in enumerate(train_loader):\n",
        "        # Data for training the discriminator\n",
        "        real_samples_labels = torch.ones((batch_size, 1),dtype=torch.cfloat)\n",
        "        latent_space_samples = torch.randn((batch_size, 2),dtype=torch.cfloat)\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        generated_samples_labels = torch.zeros((batch_size, 1),dtype=torch.cfloat)\n",
        "        all_samples = torch.cat((real_samples, generated_samples))\n",
        "        all_samples_labels = torch.cat(\n",
        "            (real_samples_labels, generated_samples_labels)\n",
        "        )\n",
        "        discriminator.zero_grad()\n",
        "        output_discriminator = discriminator(all_samples)\n",
        "\n",
        "        # Training the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        output_discriminator = discriminator(all_samples)\n",
        "        loss_discriminator = loss_function(\n",
        "            output_discriminator, all_samples_labels)\n",
        "        loss_discriminator.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        # Data for training the generator\n",
        "        latent_space_samples = torch.randn((batch_size, 2),dtype=torch.cfloat)\n",
        "\n",
        "        # Training the generator\n",
        "        generator.zero_grad()\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        output_discriminator_generated = discriminator(generated_samples)\n",
        "        loss_generator = loss_function(\n",
        "            output_discriminator_generated, real_samples_labels\n",
        "        )\n",
        "        loss_generator.backward()\n"
      ],
      "metadata": {
        "id": "bB8pFrhfaqwL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        optimizer_generator.step()\n",
        "\n",
        "        # Show loss\n",
        "        if epoch % 10 == 0 and n == batch_size - 1:\n",
        "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
        "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")"
      ],
      "metadata": {
        "id": "1nmIl4FUa4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_space_samples = torch.randn(100, 2,dtype=torch.cfloat)\n",
        "generated_samples = generator(latent_space_samples)"
      ],
      "metadata": {
        "id": "7QuRQKwfZe04"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(latent_space_samples)"
      ],
      "metadata": {
        "id": "yo3nTK2ZavLb",
        "outputId": "ef1bc402-f247-4793-e29e-52051eff8522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj]], grad_fn=<ViewAsComplexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_space_samples"
      ],
      "metadata": {
        "id": "nnndzVmjaDPb",
        "outputId": "5ae6969a-3002-4921-8b65-b8f656d3e300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.9374e-02+3.3002e-02j,  1.1338e-01+7.0591e-05j],\n",
              "        [ 1.2707e+00+5.4934e-01j,  8.3832e-01-9.9213e-01j],\n",
              "        [-2.5704e-01+3.5390e-02j, -1.9093e-01+1.7553e-01j],\n",
              "        [-5.7357e-01+1.1355e+00j, -9.5251e-01+9.8376e-01j],\n",
              "        [ 2.5741e-01+6.7453e-01j, -1.4936e+00-2.3991e-01j],\n",
              "        [-9.1279e-02-3.4159e-01j,  1.1577e+00+6.6517e-01j],\n",
              "        [ 9.7756e-01-4.7562e-01j,  2.4989e-01+1.7386e-01j],\n",
              "        [-2.0137e-01-4.2738e-01j, -1.1866e+00+2.6014e-01j],\n",
              "        [-1.1717e+00+2.3350e-01j, -6.3544e-01+1.2732e+00j],\n",
              "        [-7.7418e-01-1.8462e-02j, -4.8653e-02-4.6823e-01j],\n",
              "        [ 5.8409e-01+5.8234e-01j,  1.1352e+00-6.2812e-04j],\n",
              "        [ 1.1030e+00+7.2447e-02j,  1.0657e-01-2.8100e-01j],\n",
              "        [-1.4806e+00-5.9015e-01j,  5.1509e-02-2.0712e-01j],\n",
              "        [-5.2135e-01+9.4070e-01j,  7.4988e-02-3.9318e-01j],\n",
              "        [ 1.1874e+00-3.2887e-01j, -3.4695e-01+3.9471e-01j],\n",
              "        [ 5.9014e-01+4.7772e-01j, -5.9292e-01-1.4757e+00j],\n",
              "        [ 1.2999e+00-1.4615e-01j,  6.1669e-01+2.5908e-01j],\n",
              "        [-2.0201e-01+5.0317e-02j, -2.8003e-01+6.5725e-01j],\n",
              "        [-8.8370e-01-1.2004e+00j,  6.5774e-01-4.4317e-01j],\n",
              "        [ 8.5061e-02-9.7227e-01j, -5.4960e-01-4.3942e-01j],\n",
              "        [ 1.1480e+00-1.6241e+00j,  3.8369e-01-6.9646e-01j],\n",
              "        [-7.5208e-01+1.7295e+00j,  2.4915e-01-2.3241e-02j],\n",
              "        [-4.9833e-01+9.1329e-01j,  2.1846e-01+1.1805e+00j],\n",
              "        [-2.0234e-01+8.3649e-01j, -7.2247e-01-1.7098e-01j],\n",
              "        [ 1.4377e+00-6.9532e-01j,  6.6254e-01+8.2318e-01j],\n",
              "        [ 1.0853e+00+1.1111e-01j,  6.8715e-01-2.9445e-01j],\n",
              "        [-1.3574e+00+2.1687e-01j,  4.3416e-01+3.9037e-01j],\n",
              "        [-4.0334e-03-2.2105e-01j, -1.6943e-01-3.1686e-01j],\n",
              "        [ 3.9221e-01+8.6516e-01j,  1.2224e+00-3.0940e-01j],\n",
              "        [ 2.1794e-01+8.6791e-01j, -6.5358e-02-3.0670e-01j],\n",
              "        [ 5.3893e-01+3.4609e-01j, -2.2909e-01+1.5175e-01j],\n",
              "        [ 2.6895e-01+5.0166e-01j,  6.2702e-01-4.8826e-01j],\n",
              "        [ 1.5800e-01-3.9233e-02j,  1.6989e-01+1.4434e+00j],\n",
              "        [-1.0703e+00-3.2925e-01j,  6.4904e-01-4.2010e-01j],\n",
              "        [-1.0482e+00-1.3850e+00j, -6.5227e-01+7.9139e-01j],\n",
              "        [ 3.0752e-01+1.9161e+00j, -4.4704e-01+2.1105e-01j],\n",
              "        [-2.6493e-01+4.0709e-01j,  6.7035e-01-6.0391e-01j],\n",
              "        [-1.9277e+00-1.0153e+00j,  5.4722e-02-1.3324e+00j],\n",
              "        [ 4.2544e-01-6.5622e-01j, -1.0765e+00+9.4546e-01j],\n",
              "        [-2.2674e-02+3.7084e-01j, -7.4733e-01+3.5587e-01j],\n",
              "        [ 1.7516e-01-1.4980e-01j, -3.7816e-01+4.9350e-02j],\n",
              "        [ 8.9172e-02-2.2149e-01j, -2.8352e-01+2.3142e-01j],\n",
              "        [ 3.5043e-01-4.3326e-01j,  2.5368e-01+1.8196e-01j],\n",
              "        [ 3.9382e-01-1.0017e-01j,  2.5033e-01-8.9037e-02j],\n",
              "        [ 1.2187e+00+7.1535e-01j,  7.0727e-02-1.6580e-01j],\n",
              "        [ 3.1190e-01-4.4687e-01j,  4.0344e-01-3.8079e-01j],\n",
              "        [-5.5193e-01-4.6216e-01j,  4.7669e-01+7.7872e-02j],\n",
              "        [-7.1724e-01+2.0178e-01j,  3.9364e-01+3.8733e-01j],\n",
              "        [-5.5292e-01+1.8430e-01j,  4.7520e-01-8.2335e-01j],\n",
              "        [ 9.7698e-01+9.3740e-01j, -9.2920e-02+2.9303e-02j],\n",
              "        [ 4.0126e-01-3.2525e-01j,  9.1164e-02+1.0226e+00j],\n",
              "        [-5.5911e-01+9.0438e-02j,  5.0954e-01+4.5311e-02j],\n",
              "        [ 2.0804e-01+8.6813e-01j,  9.6387e-02-1.0207e+00j],\n",
              "        [-2.6263e-01-8.4866e-01j,  1.6898e-01+4.9987e-01j],\n",
              "        [-2.5544e-01+1.7373e-01j, -5.8766e-01-9.4082e-02j],\n",
              "        [-4.4507e-02-8.5048e-01j,  3.1271e-02+2.2496e-01j],\n",
              "        [ 1.1630e+00+1.0500e-01j, -4.0074e-01-1.4672e+00j],\n",
              "        [-6.6639e-01-3.4775e-01j, -6.4444e-01+6.5346e-01j],\n",
              "        [ 1.4919e+00-4.5161e-01j,  1.5202e-01+1.3799e-01j],\n",
              "        [ 2.3044e-01+3.1613e-03j,  9.1359e-01-2.1465e+00j],\n",
              "        [-3.4657e-01+3.3169e-01j,  4.4933e-01+1.8702e+00j],\n",
              "        [ 9.8878e-01-9.9463e-01j,  5.3654e-02+1.5059e-01j],\n",
              "        [ 3.2601e-01-2.2167e-01j, -1.6318e+00+5.6557e-01j],\n",
              "        [-5.1372e-01+7.4540e-02j,  1.0726e+00-3.3240e-01j],\n",
              "        [-3.7545e-01-1.0271e+00j, -9.8848e-01-3.6466e-01j],\n",
              "        [ 2.1002e-01+3.6507e-01j, -1.1750e+00-1.2290e+00j],\n",
              "        [-6.5362e-02+9.9022e-01j, -6.1695e-01+1.3072e-01j],\n",
              "        [ 9.7521e-02-1.1322e+00j,  5.2228e-01-3.1941e-01j],\n",
              "        [ 2.9479e-01+8.1532e-01j, -5.8855e-02-2.4592e-01j],\n",
              "        [ 1.9393e+00+1.6177e-01j, -9.1078e-01-3.0722e-01j],\n",
              "        [-6.4695e-02-4.7781e-01j, -6.4567e-01+1.0877e-01j],\n",
              "        [ 5.8266e-02-8.4921e-01j,  2.6051e-01-1.9832e+00j],\n",
              "        [-1.1621e-01+7.1954e-01j,  1.2128e-02+2.3461e-01j],\n",
              "        [ 1.5084e-01-9.8713e-01j, -7.7459e-01+1.4440e+00j],\n",
              "        [-2.9182e-01+2.7554e-01j, -4.1876e-01+6.6738e-01j],\n",
              "        [-7.8890e-01+8.5878e-01j, -9.7370e-01-3.8508e-03j],\n",
              "        [-1.2344e-01+1.0760e+00j,  3.1066e-01-9.4286e-01j],\n",
              "        [-9.9610e-02+2.4082e+00j,  6.1358e-01-1.2628e+00j],\n",
              "        [ 9.6328e-01+1.4785e-01j, -7.1078e-01-8.7014e-01j],\n",
              "        [ 7.0601e-01-6.7821e-02j, -6.8450e-01-4.6829e-01j],\n",
              "        [ 5.6153e-01-7.7647e-01j,  2.1929e-01+1.7382e+00j],\n",
              "        [-6.2999e-01+6.3361e-01j, -6.4640e-01-4.5872e-01j],\n",
              "        [ 7.9078e-01-6.3379e-01j,  5.1618e-01-7.8515e-01j],\n",
              "        [ 4.1092e-01-8.9099e-01j, -7.0782e-01-4.4883e-01j],\n",
              "        [ 1.6326e-01+3.5517e-01j,  1.2760e+00-9.5742e-02j],\n",
              "        [ 2.3314e+00+2.4433e-01j, -1.3277e-01+6.5200e-01j],\n",
              "        [-2.8749e-01-1.9888e+00j, -4.9670e-01-9.1981e-01j],\n",
              "        [ 1.4303e+00+1.0634e+00j,  1.4631e-01-3.3407e-01j],\n",
              "        [ 7.6822e-01+1.0867e+00j, -4.0039e-01+5.9938e-01j],\n",
              "        [ 1.4320e-01-9.5013e-01j, -1.0550e+00+2.6286e-01j],\n",
              "        [-6.7685e-01-5.1880e-01j,  9.4485e-01-4.8729e-01j],\n",
              "        [ 2.5964e-01+2.1070e-01j,  1.2976e-01+8.1920e-01j],\n",
              "        [-5.8466e-02+3.6969e-01j, -2.8142e-01-5.0031e-02j],\n",
              "        [-4.0389e-01+2.8609e-01j,  1.4188e+00+6.2023e-01j],\n",
              "        [-1.4347e+00-1.2718e+00j,  2.2744e-01-1.5499e+00j],\n",
              "        [-1.1795e-01-8.8598e-02j, -1.1924e+00-2.7770e-01j],\n",
              "        [-2.3273e-01+8.7586e-01j, -1.8731e-01+6.2490e-01j],\n",
              "        [ 3.1179e-01-9.5779e-01j,  3.7538e-01-1.9811e-01j],\n",
              "        [-1.3790e+00+2.5581e-01j,  2.7728e-01+9.1551e-01j],\n",
              "        [ 1.4579e-01+2.1223e-01j, -2.0121e-03-1.3329e+00j]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_samples = generated_samples.detach()\n",
        "plt.plot(np.real(generated_samples[:, 1]), np.imag(generated_samples[:, 1]), \".\")"
      ],
      "metadata": {
        "id": "CRnp6i4LZwvA",
        "outputId": "3d3ca9b1-797b-4a66-dc9d-7fea477306ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f36e2c96250>]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_samples"
      ],
      "metadata": {
        "id": "ItCJiprIapiZ",
        "outputId": "185939e1-b6ec-45cb-a9aa-2bd2d54ee082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj],\n",
              "        [nan+nanj, nan+nanj]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}