{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_Pytorch-Complex.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP8OnMh0qj91RuKHbV8ZH+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emanbuc/ML-GAN/blob/main/Test_Pytorch_Complex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "TvcdrjVLWgE7",
        "outputId": "dce0a22c-8fd5-44f6-e6ea-abb8e0fa1899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.2)\n",
            "Collecting torch\n",
            "  Using cached torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.2\n",
            "    Uninstalling torch-1.10.2:\n",
            "      Successfully uninstalled torch-1.10.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.3 requires torch==1.10.2, but you have torch 1.11.0 which is incompatible.\n",
            "fastai 2.5.3 requires torch<1.11,>=1.7.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-complex in /usr/local/lib/python3.7/dist-packages (0.0.8)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from pytorch-complex) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->pytorch-complex) (4.2.0)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --upgrade\n",
        "!pip install pytorch-complex\n",
        "!pip install fastai==2.5.3 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import  torchcomplex.nn as nnc\n",
        "import torch\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "copiIj5YW5AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This discriminator is an MLP neural network \n",
        "class Discriminator(nnc.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nnc.Sequential(\n",
        "            #The input is two-dimensional, and the first hidden layer is composed \n",
        "            # of 256 neurons with ReLU activation.\n",
        "            nnc.Linear(2, 256),\n",
        "            nnc.CReLU(),\n",
        "            # use dropout after hidden layer to avoid overfitting.\n",
        "            nnc.Dropout(0.3),\n",
        "            nnc.Linear(256, 128),\n",
        "            nnc.CReLU(),\n",
        "            nnc.Dropout(0.3),\n",
        "            nnc.Linear(128, 64),\n",
        "            nnc.CReLU(),\n",
        "            nnc.Dropout(0.3),\n",
        "            nnc.Linear(64, 1),\n",
        "            # The output is composed of a single neuron with sigmoidal activation\n",
        "            # to represent a probability.\n",
        "            nnc.Sigmoid(),\n",
        "        )\n",
        "\n",
        "     # .forward() to describe how the output of the model is calculated\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "# instantiate a Discriminator object\n",
        "discriminator = Discriminator()"
      ],
      "metadata": {
        "id": "VU0xri7QXSSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nnc.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nnc.Sequential(\n",
        "            nnc.Linear(2, 16),\n",
        "            nnc.CReLU(),\n",
        "            nnc.Linear(16, 32),\n",
        "            nnc.CReLU(),\n",
        "            nnc.Linear(32, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "generator = Generator()"
      ],
      "metadata": {
        "id": "EeyPZdGoZ9F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "num_epochs = 300\n",
        "\n",
        "# The binary cross-entropy function is a suitable loss function for training the discriminator \n",
        "# because it considers a binary classification task. \n",
        "# Itâ€™s also suitable for training the generator since it feeds its output to the discriminator,\n",
        "# which provides a binary observable output.\n",
        "#loss_function = torch.nn.BCELoss()\n",
        "# ne serve una che support ai numeri complessi\n",
        "loss_function = torch.nn.L1Loss()"
      ],
      "metadata": {
        "id": "3qkawDJnai_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "JFK49AZvamBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Training Data from EIS Dataset"
      ],
      "metadata": {
        "id": "W_4LyFCya4fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from fastai.vision.all import *\n",
        "import sys"
      ],
      "metadata": {
        "id": "ByBhkLHqa7kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load EB_ML python libraries\n",
        "# The following libraries are used in this notebook and should be installed in your local machine before running this notebook.\n",
        "# eb_colab_utils.py\n",
        "# eb_ml_battery_lib.py\n",
        "# eb_ml_utils.py\n",
        "\n",
        "# path to load external *.py files used in this notebook\n",
        "# Note: in Google Colab virtual machine you shoud copy the files in \"/content\" folder after BEFORE running this notebook's cell\n",
        "external_python_file_path=\"'/.'\"\n",
        "sys.path.append(external_python_file_path)\n",
        "\n",
        "\n",
        "from eb_ml_colab_utils import get_root_path,copy_model_to_google_drive\n",
        "from eb_ml_battery_lib import load_soc_dataset,generate_image_files_from_eis\n",
        "from eb_ml_utils import save_model_weights,build_data_loader,build_and_train_learner,score_model"
      ],
      "metadata": {
        "id": "HblKeAioa9w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#configuration dictionary\n",
        "config ={}\n",
        "\n",
        "# Root working folder (local or Google Drive)\n",
        "# config['ROOT_DIR'] = get_root_path(\"batterie\")\n",
        "config['ROOT_DIR'] = get_root_path(\"batterie\")  \n",
        "\n",
        "# Folder with dataset in CSV format\n",
        "#config['DATASETS_DIR'] = config['ROOT_DIR']+\"/datasets\"\n",
        "config['DATASETS_DIR'] = config['ROOT_DIR']+\"/datasets/EIS-vs-SOC-2022\"\n",
        "\n",
        "# List of SoC level into dataset\n",
        "#config['soc_list']=['100','090','080','070','060','050','040','030','020','010']\n",
        "config['soc_list']=['100','090','080','070','060','050','040','030','020','010']\n",
        "\n",
        "\n",
        "# Folder to store trained model\n",
        "#config['MODELS_DIR'] = config['ROOT_DIR']+\"/models\"\n",
        "config['MODELS_DIR'] = config['ROOT_DIR']+\"/models\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq1sfYt-bF-l",
        "outputId": "d0c4e004-d06b-4b3c-ab30-a38bfd1b2c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on COLAB\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data acquition file to load from dateset folder\n",
        "# EIS Dataset https://data.mendeley.com/datasets/ch3sydbbrg/2\n",
        "frequency_list=[ 0.05, 0.1, 0.2, 0.4, 1, 2, 4, 10, 20, 40, 100, 200, 400, 1000]\n",
        "battery_list=[1,2,3,4,5,7,8,9,10,11,12] # Data acquitions 6,13 to be used for TEST]\n",
        "dataset,feature_col_names=load_soc_dataset(battery_list,config[\"soc_list\"],config['DATASETS_DIR'])"
      ],
      "metadata": {
        "id": "rCde4RBybIwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_col_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJYjcl-GbLLT",
        "outputId": "ffd41629-80f1-44bf-ff9b-79a843768662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Z_f0',\n",
              " 'Z_f1',\n",
              " 'Z_f2',\n",
              " 'Z_f3',\n",
              " 'Z_f4',\n",
              " 'Z_f5',\n",
              " 'Z_f6',\n",
              " 'Z_f7',\n",
              " 'Z_f8',\n",
              " 'Z_f9',\n",
              " 'Z_f10',\n",
              " 'Z_f11',\n",
              " 'Z_f12',\n",
              " 'Z_f13']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soc_10_training=dataset.query('SOC== \"010\"')[feature_col_names]\n",
        "soc_10_training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "s_xv0MZBbOQc",
        "outputId": "b951813b-85e8-4123-c8df-d89c1db700c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Z_f0                Z_f1                Z_f2  \\\n",
              "010  0.079771-0.005349j  0.077740-0.004683j  0.076173-0.003523j   \n",
              "010  0.078894-0.004942j  0.077379-0.003965j  0.076076-0.003162j   \n",
              "010  0.074923-0.004870j  0.073477-0.003759j  0.072532-0.002710j   \n",
              "010  0.074411-0.004880j  0.072787-0.003883j  0.071541-0.003067j   \n",
              "010  0.078197-0.004944j  0.076708-0.003921j  0.075516-0.002831j   \n",
              "010  0.073499-0.004352j  0.071975-0.003375j  0.071127-0.002276j   \n",
              "010  0.075002-0.004450j  0.073594-0.003316j  0.072728-0.002386j   \n",
              "010  0.079897-0.004582j  0.078391-0.003490j  0.077319-0.002565j   \n",
              "010  0.077036-0.004560j  0.075534-0.003552j  0.074379-0.002565j   \n",
              "010  0.097612-0.004647j  0.095967-0.003856j  0.094797-0.002671j   \n",
              "010  0.071875-0.003984j  0.070516-0.003260j  0.069583-0.002225j   \n",
              "\n",
              "                   Z_f3                Z_f4                Z_f5  \\\n",
              "010  0.075399-0.003229j  0.073304-0.003548j  0.071919-0.004337j   \n",
              "010  0.075439-0.002577j  0.073901-0.002919j  0.072642-0.003744j   \n",
              "010  0.071758-0.002316j  0.070510-0.002377j  0.069565-0.003037j   \n",
              "010  0.070933-0.002429j  0.069589-0.002628j  0.068560-0.003292j   \n",
              "010  0.074972-0.002334j  0.073589-0.002613j  0.072889-0.003227j   \n",
              "010  0.070579-0.001976j  0.069628-0.001913j  0.069061-0.002316j   \n",
              "010  0.072421-0.002074j  0.071293-0.002048j  0.070492-0.002500j   \n",
              "010  0.076870-0.001969j  0.075815-0.002046j  0.075152-0.002592j   \n",
              "010  0.073773-0.002133j  0.072797-0.002270j  0.072098-0.002704j   \n",
              "010  0.094201-0.002246j  0.093193-0.002260j  0.092317-0.002724j   \n",
              "010  0.069102-0.001781j  0.068355-0.001730j  0.067728-0.002101j   \n",
              "\n",
              "                   Z_f6                Z_f7                Z_f8  \\\n",
              "010  0.069978-0.004903j  0.066565-0.005511j  0.063821-0.005493j   \n",
              "010  0.070944-0.004596j  0.067612-0.005322j  0.064984-0.005585j   \n",
              "010  0.068364-0.003993j  0.065447-0.005366j  0.062517-0.006183j   \n",
              "010  0.067239-0.003997j  0.064362-0.005227j  0.061435-0.005945j   \n",
              "010  0.071461-0.004222j  0.068578-0.005554j  0.065428-0.006255j   \n",
              "010  0.068349-0.002973j  0.066075-0.004737j  0.063371-0.005850j   \n",
              "010  0.069626-0.003237j  0.067319-0.004893j  0.064682-0.005829j   \n",
              "010  0.074303-0.003259j  0.071812-0.004872j  0.069079-0.005790j   \n",
              "010  0.071250-0.003512j  0.068566-0.005268j  0.065698-0.006345j   \n",
              "010  0.091361-0.003504j  0.088793-0.004872j  0.086085-0.005751j   \n",
              "010  0.066984-0.002790j  0.065122-0.004238j  0.062783-0.005229j   \n",
              "\n",
              "                   Z_f9               Z_f10               Z_f11  \\\n",
              "010  0.061411-0.005366j  0.057958-0.005174j  0.055633-0.004818j   \n",
              "010  0.062448-0.005382j  0.059086-0.005205j  0.056763-0.004722j   \n",
              "010  0.059429-0.006285j  0.055555-0.005599j  0.053129-0.004991j   \n",
              "010  0.058558-0.005913j  0.054922-0.005389j  0.052485-0.004896j   \n",
              "010  0.062342-0.006436j  0.058299-0.005866j  0.055709-0.005264j   \n",
              "010  0.060204-0.006134j  0.056277-0.005871j  0.053678-0.004992j   \n",
              "010  0.061710-0.006009j  0.057768-0.005385j  0.055370-0.004826j   \n",
              "010  0.065966-0.006023j  0.062122-0.005708j  0.059560-0.004794j   \n",
              "010  0.062095-0.006770j  0.057652-0.006236j  0.054922-0.005423j   \n",
              "010  0.083177-0.006003j  0.079377-0.005685j  0.076842-0.005000j   \n",
              "010  0.059880-0.005821j  0.056042-0.005589j  0.053541-0.004915j   \n",
              "\n",
              "                  Z_f12               Z_f13  \n",
              "010  0.053745-0.004118j  0.051447-0.003001j  \n",
              "010  0.054563-0.004204j  0.052500-0.002999j  \n",
              "010  0.050902-0.004287j  0.048653-0.003007j  \n",
              "010  0.050414-0.004192j  0.048036-0.003069j  \n",
              "010  0.053496-0.004382j  0.051192-0.003051j  \n",
              "010  0.051553-0.004286j  0.049336-0.002925j  \n",
              "010  0.053423-0.003888j  0.051324-0.002779j  \n",
              "010  0.057592-0.003948j  0.055631-0.002843j  \n",
              "010  0.052739-0.004440j  0.050409-0.003261j  \n",
              "010  0.074681-0.004194j  0.072418-0.002935j  \n",
              "010  0.051409-0.004200j  0.049157-0.002987j  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53a855ae-0d44-46a4-a857-100cc7d5a1d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Z_f0</th>\n",
              "      <th>Z_f1</th>\n",
              "      <th>Z_f2</th>\n",
              "      <th>Z_f3</th>\n",
              "      <th>Z_f4</th>\n",
              "      <th>Z_f5</th>\n",
              "      <th>Z_f6</th>\n",
              "      <th>Z_f7</th>\n",
              "      <th>Z_f8</th>\n",
              "      <th>Z_f9</th>\n",
              "      <th>Z_f10</th>\n",
              "      <th>Z_f11</th>\n",
              "      <th>Z_f12</th>\n",
              "      <th>Z_f13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.079771-0.005349j</td>\n",
              "      <td>0.077740-0.004683j</td>\n",
              "      <td>0.076173-0.003523j</td>\n",
              "      <td>0.075399-0.003229j</td>\n",
              "      <td>0.073304-0.003548j</td>\n",
              "      <td>0.071919-0.004337j</td>\n",
              "      <td>0.069978-0.004903j</td>\n",
              "      <td>0.066565-0.005511j</td>\n",
              "      <td>0.063821-0.005493j</td>\n",
              "      <td>0.061411-0.005366j</td>\n",
              "      <td>0.057958-0.005174j</td>\n",
              "      <td>0.055633-0.004818j</td>\n",
              "      <td>0.053745-0.004118j</td>\n",
              "      <td>0.051447-0.003001j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.078894-0.004942j</td>\n",
              "      <td>0.077379-0.003965j</td>\n",
              "      <td>0.076076-0.003162j</td>\n",
              "      <td>0.075439-0.002577j</td>\n",
              "      <td>0.073901-0.002919j</td>\n",
              "      <td>0.072642-0.003744j</td>\n",
              "      <td>0.070944-0.004596j</td>\n",
              "      <td>0.067612-0.005322j</td>\n",
              "      <td>0.064984-0.005585j</td>\n",
              "      <td>0.062448-0.005382j</td>\n",
              "      <td>0.059086-0.005205j</td>\n",
              "      <td>0.056763-0.004722j</td>\n",
              "      <td>0.054563-0.004204j</td>\n",
              "      <td>0.052500-0.002999j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.074923-0.004870j</td>\n",
              "      <td>0.073477-0.003759j</td>\n",
              "      <td>0.072532-0.002710j</td>\n",
              "      <td>0.071758-0.002316j</td>\n",
              "      <td>0.070510-0.002377j</td>\n",
              "      <td>0.069565-0.003037j</td>\n",
              "      <td>0.068364-0.003993j</td>\n",
              "      <td>0.065447-0.005366j</td>\n",
              "      <td>0.062517-0.006183j</td>\n",
              "      <td>0.059429-0.006285j</td>\n",
              "      <td>0.055555-0.005599j</td>\n",
              "      <td>0.053129-0.004991j</td>\n",
              "      <td>0.050902-0.004287j</td>\n",
              "      <td>0.048653-0.003007j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.074411-0.004880j</td>\n",
              "      <td>0.072787-0.003883j</td>\n",
              "      <td>0.071541-0.003067j</td>\n",
              "      <td>0.070933-0.002429j</td>\n",
              "      <td>0.069589-0.002628j</td>\n",
              "      <td>0.068560-0.003292j</td>\n",
              "      <td>0.067239-0.003997j</td>\n",
              "      <td>0.064362-0.005227j</td>\n",
              "      <td>0.061435-0.005945j</td>\n",
              "      <td>0.058558-0.005913j</td>\n",
              "      <td>0.054922-0.005389j</td>\n",
              "      <td>0.052485-0.004896j</td>\n",
              "      <td>0.050414-0.004192j</td>\n",
              "      <td>0.048036-0.003069j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.078197-0.004944j</td>\n",
              "      <td>0.076708-0.003921j</td>\n",
              "      <td>0.075516-0.002831j</td>\n",
              "      <td>0.074972-0.002334j</td>\n",
              "      <td>0.073589-0.002613j</td>\n",
              "      <td>0.072889-0.003227j</td>\n",
              "      <td>0.071461-0.004222j</td>\n",
              "      <td>0.068578-0.005554j</td>\n",
              "      <td>0.065428-0.006255j</td>\n",
              "      <td>0.062342-0.006436j</td>\n",
              "      <td>0.058299-0.005866j</td>\n",
              "      <td>0.055709-0.005264j</td>\n",
              "      <td>0.053496-0.004382j</td>\n",
              "      <td>0.051192-0.003051j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.073499-0.004352j</td>\n",
              "      <td>0.071975-0.003375j</td>\n",
              "      <td>0.071127-0.002276j</td>\n",
              "      <td>0.070579-0.001976j</td>\n",
              "      <td>0.069628-0.001913j</td>\n",
              "      <td>0.069061-0.002316j</td>\n",
              "      <td>0.068349-0.002973j</td>\n",
              "      <td>0.066075-0.004737j</td>\n",
              "      <td>0.063371-0.005850j</td>\n",
              "      <td>0.060204-0.006134j</td>\n",
              "      <td>0.056277-0.005871j</td>\n",
              "      <td>0.053678-0.004992j</td>\n",
              "      <td>0.051553-0.004286j</td>\n",
              "      <td>0.049336-0.002925j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.075002-0.004450j</td>\n",
              "      <td>0.073594-0.003316j</td>\n",
              "      <td>0.072728-0.002386j</td>\n",
              "      <td>0.072421-0.002074j</td>\n",
              "      <td>0.071293-0.002048j</td>\n",
              "      <td>0.070492-0.002500j</td>\n",
              "      <td>0.069626-0.003237j</td>\n",
              "      <td>0.067319-0.004893j</td>\n",
              "      <td>0.064682-0.005829j</td>\n",
              "      <td>0.061710-0.006009j</td>\n",
              "      <td>0.057768-0.005385j</td>\n",
              "      <td>0.055370-0.004826j</td>\n",
              "      <td>0.053423-0.003888j</td>\n",
              "      <td>0.051324-0.002779j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.079897-0.004582j</td>\n",
              "      <td>0.078391-0.003490j</td>\n",
              "      <td>0.077319-0.002565j</td>\n",
              "      <td>0.076870-0.001969j</td>\n",
              "      <td>0.075815-0.002046j</td>\n",
              "      <td>0.075152-0.002592j</td>\n",
              "      <td>0.074303-0.003259j</td>\n",
              "      <td>0.071812-0.004872j</td>\n",
              "      <td>0.069079-0.005790j</td>\n",
              "      <td>0.065966-0.006023j</td>\n",
              "      <td>0.062122-0.005708j</td>\n",
              "      <td>0.059560-0.004794j</td>\n",
              "      <td>0.057592-0.003948j</td>\n",
              "      <td>0.055631-0.002843j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.077036-0.004560j</td>\n",
              "      <td>0.075534-0.003552j</td>\n",
              "      <td>0.074379-0.002565j</td>\n",
              "      <td>0.073773-0.002133j</td>\n",
              "      <td>0.072797-0.002270j</td>\n",
              "      <td>0.072098-0.002704j</td>\n",
              "      <td>0.071250-0.003512j</td>\n",
              "      <td>0.068566-0.005268j</td>\n",
              "      <td>0.065698-0.006345j</td>\n",
              "      <td>0.062095-0.006770j</td>\n",
              "      <td>0.057652-0.006236j</td>\n",
              "      <td>0.054922-0.005423j</td>\n",
              "      <td>0.052739-0.004440j</td>\n",
              "      <td>0.050409-0.003261j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.097612-0.004647j</td>\n",
              "      <td>0.095967-0.003856j</td>\n",
              "      <td>0.094797-0.002671j</td>\n",
              "      <td>0.094201-0.002246j</td>\n",
              "      <td>0.093193-0.002260j</td>\n",
              "      <td>0.092317-0.002724j</td>\n",
              "      <td>0.091361-0.003504j</td>\n",
              "      <td>0.088793-0.004872j</td>\n",
              "      <td>0.086085-0.005751j</td>\n",
              "      <td>0.083177-0.006003j</td>\n",
              "      <td>0.079377-0.005685j</td>\n",
              "      <td>0.076842-0.005000j</td>\n",
              "      <td>0.074681-0.004194j</td>\n",
              "      <td>0.072418-0.002935j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.071875-0.003984j</td>\n",
              "      <td>0.070516-0.003260j</td>\n",
              "      <td>0.069583-0.002225j</td>\n",
              "      <td>0.069102-0.001781j</td>\n",
              "      <td>0.068355-0.001730j</td>\n",
              "      <td>0.067728-0.002101j</td>\n",
              "      <td>0.066984-0.002790j</td>\n",
              "      <td>0.065122-0.004238j</td>\n",
              "      <td>0.062783-0.005229j</td>\n",
              "      <td>0.059880-0.005821j</td>\n",
              "      <td>0.056042-0.005589j</td>\n",
              "      <td>0.053541-0.004915j</td>\n",
              "      <td>0.051409-0.004200j</td>\n",
              "      <td>0.049157-0.002987j</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53a855ae-0d44-46a4-a857-100cc7d5a1d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-53a855ae-0d44-46a4-a857-100cc7d5a1d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-53a855ae-0d44-46a4-a857-100cc7d5a1d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_EIS_num = len(soc_10_training)\n",
        "n_frequency =len(frequency_list)\n",
        "train_data_length = total_EIS_num * n_frequency  #14*11\n",
        "\n",
        "# initialize train_data, a tensor with dimensions of <train_data_length> rows and 2 columns, all containing zeros. \n",
        "# Note: A tensor is a multidimensional array similar to a NumPy array.\n",
        "train_data = torch.empty((train_data_length,2),dtype=torch.cfloat)"
      ],
      "metadata": {
        "id": "atINl46EbRMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import mod\n",
        "\n",
        "for i  in range(0,train_data_length):\n",
        "  f_index = i % 14\n",
        "  row_index = i // 14\n",
        "  # the first column of train_data store the frequency values\n",
        "  train_data[i, 0] = frequency_list[f_index]\n",
        "  # the second column of the tensor  Z(f)\n",
        "  train_data[i, 1] = tensor(soc_10_training)[row_index,f_index]"
      ],
      "metadata": {
        "id": "wVI6eFM0bXh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init tensor of labels, which are required by PyTorchâ€™s data loader. \n",
        "# Since GANs make use of unsupervised learning techniques, the labels can be anything.\n",
        "train_labels = torch.zeros(train_data_length)\n",
        "\n",
        "# create train_set as a list of tuples, with each row of train_data and train_labels represented in each tuple \n",
        "# this is the format expected by PyTorchâ€™s data loader.\n",
        "train_set = [\n",
        "    (train_data[i], train_labels[i]) for i in range(train_data_length)\n",
        "]"
      ],
      "metadata": {
        "id": "W4ob0w25bam0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "WuCmtXaYkh2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch data loader called train_loader.\n",
        "# Data Loader will shuffle the data from train_set and return batches of <batch_size> samples \n",
        "# to be used to train the neural networks.\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "6CyuEAJUbAwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for n, (real_samples, _) in enumerate(train_loader):\n",
        "        # Data for training the discriminator\n",
        "        real_samples_labels = torch.ones((batch_size, 1),dtype=torch.cfloat)\n",
        "        latent_space_samples = torch.randn((batch_size, 2),dtype=torch.cfloat)\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        generated_samples_labels = torch.zeros((batch_size, 1),dtype=torch.cfloat)\n",
        "        all_samples = torch.cat((real_samples, generated_samples))\n",
        "        all_samples_labels = torch.cat(\n",
        "            (real_samples_labels, generated_samples_labels)\n",
        "        )\n",
        "\n",
        "        # Training the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        output_discriminator = discriminator(all_samples)\n",
        "        loss_discriminator = loss_function(\n",
        "            output_discriminator, all_samples_labels)\n",
        "        loss_discriminator.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        # Data for training the generator\n",
        "        latent_space_samples = torch.randn((batch_size, 2),dtype=torch.cfloat)\n",
        "\n",
        "        # Training the generator\n",
        "        generator.zero_grad()\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        output_discriminator_generated = discriminator(generated_samples)\n",
        "        loss_generator = loss_function(\n",
        "            output_discriminator_generated, real_samples_labels\n",
        "        )\n",
        "        loss_generator.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        # Show loss\n",
        "        if epoch % 10 == 0 and n == batch_size - 1:\n",
        "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
        "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "bB8pFrhfaqwL",
        "outputId": "33dc8930-81e1-4270-f1a0-e0615e485cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([58, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-64d75fd9b57b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutput_discriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         loss_discriminator = loss_function(\n\u001b[0;32m---> 17\u001b[0;31m             output_discriminator, all_samples_labels)\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3078\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (58) must match the size of tensor b (64) at non-singleton dimension 0"
          ]
        }
      ]
    }
  ]
}