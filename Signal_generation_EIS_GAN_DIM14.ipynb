{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Signal_generation_GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhwOjApCn9k5pS2y9QsjBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emanbuc/ML-GAN/blob/main/Signal_generation_EIS_GAN_DIM14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Network - (GAN) \n",
        "\n",
        "> “Generative Adversarial Network— the most interesting idea in the last ten years in machine learning” by Yann LeCun, VP & Chief AI Scientist at Facebook, Godfather of AI.\n",
        "\n",
        "Generative Adversarial Network (GAN) is an old idea arising from the game theory, they were introduced to the machine learning community in 2014 by Ian J. Goodfellow and co-authors in the article Generative Adversarial Nets.\n",
        "\n",
        "\n",
        "Generative adversarial networks (GANs) are an exciting recent innovation in machine learning. GANs are generative models: they create new data instances that resemble your training data. For example, GANs can create images that look like photographs of human faces, even though the faces don't belong to any real person.\n",
        "\n",
        "![Images generated by a GAN created by NVIDIA](https://developers.google.com/machine-learning/gan/images/gan_faces.png)\n",
        "Figure 1: [Images generated by a GAN created by NVIDIA.](https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf)\n"
      ],
      "metadata": {
        "id": "pFlMJF1js6jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Model\n",
        "\n",
        "\"Generative\" describes a class of statistical models that contrasts with discriminative models.\n",
        "\n",
        "Informally:\n",
        "\n",
        "\n",
        "*   Generative models can generate new data \n",
        "\n",
        "*  Discriminative models discriminate between different kinds of data instances\n",
        "\n",
        "A generative model could generate new photos of animals that look like real animals, while a discriminative model could tell a dog from a cat. GANs are just one kind of generative model.\n",
        "\n",
        "More formally, given a set of data instances X and a set of labels Y:\n",
        "\n",
        "Generative models capture the joint probability p(X, Y), or just p(X) if there are no labels.\n",
        "Discriminative models capture the conditional probability p(Y | X).\n",
        "\n",
        "\n",
        "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words.\n",
        "\n",
        "A discriminative model ignores the question of whether a given instance is likely, and just tells you how likely a label is to apply to the instance.\n",
        "\n",
        "Note that this is a very general definition. There are many kinds of generative model. GANs are just one kind of generative model.\n",
        "\n"
      ],
      "metadata": {
        "id": "WEjAgvyLxpmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Game of Probabilities \n",
        "From [Generative Adversarial Network (GAN) for Dummies — A Step By Step Tutorial](https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391)"
      ],
      "metadata": {
        "id": "Rr_ToI1otYEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating new data is a game of probabilities. When we are observing the world around us and collecting data, we are performing an experiment. A simple example is taking a photo of a celebrity's face.\n",
        "\n",
        "This can be considered as a probabilistic experiment, with an unknown outcome X, also called a random variable."
      ],
      "metadata": {
        "id": "CecQ4H-ttyW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, we can define the probability that the face will be that of Tyrese, the famous celebrity singer.\n",
        "\n",
        "![](https://miro.medium.com/max/141/1*idI7l3JkxRXxoI8xiqLzlA.png)\n",
        "\n",
        "Tyrese Gibson\n",
        "All possible outcomes of such experiments build the so-called sample space, denoted Ω (all possible celebrity faces) \n",
        "\n",
        "\n",
        "![https://miro.medium.com/max/399/0*9_sm2wxAUGclWjEA.png](https://miro.medium.com/max/399/0*9_sm2wxAUGclWjEA.png)"
      ],
      "metadata": {
        "id": "IobtZjnOt3sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore we can consider probability as a function that takes an outcome, i.e. an element from the sample space (a photo) and maps the outcome to a non-negative real number so that the sum of all these numbers equals 1.\n",
        "\n",
        "We also call this a probability distribution function P(X). When we know the sample space (all possible celebrity faces) and the probability distribution (the probability of occurrence of each face), we have the full description of the experiment and we can reason about uncertainty.\n",
        "\n",
        "Generating new faces can be expressed by a random variable generation problem. The face is described by random variables, represented through its RGB values, flatten into a vector of N numbers.\n",
        "The celebrity faces are 218px height, 178px width with 3 color channels. Therefore each vector is 116412-dimensional.\n",
        "If we build a space with 116412 (N) axes, each face will be a point in that space. A celebrity-face probability distribution function P(X) would map each face to a non-negative real number so that the sum of all these numbers for all faces equals 1.\n",
        "\n",
        "Some points of that space are very likely to represent celebrity faces whereas it is highly unlikely for some others.\n",
        "\n",
        "![](https://miro.medium.com/max/645/1*XLCCQeGArsYHrcd9CJWkSg.png)\n"
      ],
      "metadata": {
        "id": "9rLHB7cLu-1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to generate random variables from complex distributions?\n",
        "\n",
        "The celebrity-face probability distribution over the N-dimensional vector space is a very complex one and we don’t know how to directly generate complex random variables.\n",
        "\n",
        "Luckily, we can represent our complex random variable by a function applied to a uniform random variable. This is the idea of the transform method. It first generates N uncorrelated uniform random variables, which is easy. It then applies a very complex function to that simple random variable! Very complex functions are naturally approximated by a neural network. After training the network will be able to take as input a simple N-dimensional uniform random variable and return another N-dimensional random variable that would follow our celebrity-face probability distribution. This is the core motivation behind generative adversarial networks.\n",
        "\n",
        "> In simple words, a GAN would generate a random variable with respect to a specific probability distribution.\n",
        "\n",
        "### Why Generative Adversarial Networks?\n",
        "\n",
        "Theoretically, we would compare the true distribution versus the generated distribution based on samples using the Maximum Mean Discrepancy (MMD) approach.\n",
        "\n",
        "This would give a distribution matching error that could be used to update the network via backpropagation. This direct method is practically very complex to implement.\n",
        "\n",
        "> Instead of directly comparing both true and generated distributions, GANs solve a non-discrimination task between true and generated samples"
      ],
      "metadata": {
        "id": "W7HddQa_vgx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative Models are HArd\n",
        "\n",
        "From : [generative-models-are-hard](https://developers.google.com/machine-learning/gan/generative#generative-models-are-hard)\n",
        "\n",
        "\n",
        "Generative models tackle a more difficult task than analogous discriminative models. Generative models have to model more.\n",
        "\n",
        "A generative model for images might capture correlations like \"things that look like boats are probably going to appear near things that look like water\" and \"eyes are unlikely to appear on foreheads.\" These are very complicated distributions.\n",
        "\n",
        "In contrast, a discriminative model might learn the difference between \"sailboat\" or \"not sailboat\" by just looking for a few tell-tale patterns. It could ignore many of the correlations that the generative model must get right.\n",
        "\n",
        "Discriminative models try to draw boundaries in the data space, while generative models try to model how data is placed throughout the space. For example, the following diagram shows discriminative and generative models of handwritten digits:\n",
        "\n",
        "![](https://developers.google.com/machine-learning/gan/images/generative_v_discriminative.png)\n",
        "\n",
        "The discriminative model tries to tell the difference between handwritten 0's and 1's by drawing a line in the data space. If it gets the line right, it can distinguish 0's from 1's without ever having to model exactly where the instances are placed in the data space on either side of the line.\n",
        "\n",
        "In contrast, the generative model tries to produce convincing 1's and 0's by generating digits that fall close to their real counterparts in the data space. It has to model the distribution throughout the data space.\n",
        "\n",
        "GANs offer an effective way to train such rich models to resemble a real distribution. To understand how they work we'll need to understand the basic structure of a GAN."
      ],
      "metadata": {
        "id": "4tg-hk7dzD-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A generative adversarial network (GAN) has two parts:\n",
        "\n",
        "- The generator learns to generate plausible data. The generated instances become negative training examples for the discriminator.\n",
        "- The discriminator learns to distinguish the generator's fake data from real data. The discriminator penalizes the generator for producing implausible results.\n",
        "\n",
        "![](https://developers.google.com/machine-learning/gan/images/gan_diagram.svg)\n",
        "\n",
        "Both the generator and the discriminator are neural networks. The generator output is connected directly to the discriminator input. Through backpropagation, the discriminator's classification provides a signal that the generator uses to update its weights.\n",
        "\n",
        "Let's explain the pieces of this system in greater detail."
      ],
      "metadata": {
        "id": "w6K65IiRz_sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Discriminator\n",
        "\n",
        "The discriminator in a GAN is simply a classifier. It tries to distinguish real data from the data created by the generator. It could use any network architecture appropriate to the type of data it's classifying.\n",
        "\n",
        "The discriminator's training data comes from two sources:\n",
        "\n",
        "- Real data instances, such as real pictures of people. The discriminator uses these instances as positive examples during training.\n",
        "- Fake data instances created by the generator. The discriminator uses these instances as negative examples during training.-\n",
        "\n"
      ],
      "metadata": {
        "id": "e1Ldyywq0fKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN Architecture\n",
        "\n",
        "A GAN has three primary components: a generator model for generating new data, a discriminator model for classifying whether generated data are real faces, or fake, and the adversarial network that pits them against each other.\n",
        "\n",
        "The generative part is responsible for taking N-dimensional uniform random variables (noise) as input and generating fake faces. The generator captures the probability P(X), where X is the input.\n",
        "The discriminative part is a simple classifier that evaluates and distinguished the generated faces from true celebrity faces. The discriminator captures the conditional probability P(Y|X), where X is the input and Y is the label."
      ],
      "metadata": {
        "id": "Va7phJWdwJaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Generative Adversarial Networks\n",
        "\n",
        "The generative network is trained to maximize the final classification error (between true and generated data), while the discriminative network is trained to minimize it. This is where the notion of adversarial networks arises from.\n",
        "From the perspective of game theory, equilibrium is reached when the generator produces samples that follow the celebrity-face probability distribution and the discriminator predicts fake or not-fake with equal probability as if it would just flip a coin.\n",
        "It is important that both networks learn equally during training and converge together. A typical situation occurs when the discriminative network becomes much better at recognizing fakes, causing the generative network to be stuck.\n",
        "\n",
        "During discriminator training, we ignore the generator loss and just use the discriminator loss, which penalizes the discriminator for misclassifying real faces as fake or generated faces as real. The generator’s weights are updated through backpropagation. Generator’s weights are not updated.\n",
        "During generator training, we use the generator loss, which penalizes the generator for failing to fool the discriminator and generating a face that the discriminator classifies as fake. The discriminator is frozen during generator training and only generator’s weights are updated through backpropagation.\n",
        "This is the magic that synthesizes celebrity faces using GANs. Convergence is often observed as fleeting, rather than stable. When you get everything right, GANs provide unbelievable results as demonstrated below.\n"
      ],
      "metadata": {
        "id": "Rn4qRafvwWdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p804XzJIttgD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MyfpDCIQiGNk"
      },
      "outputs": [],
      "source": [
        "!pip install fastai==2.5.3 -q\n",
        "import torch\n",
        "from torch import nn\n",
        "import sys\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#It’s a good practice to set up a random generator seed so that the experiment can be replicated identically on any machine. \n",
        "#To do that in PyTorch:\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PF1YmfGiN8L",
        "outputId": "ef6d5f82-292f-464a-c463-4a7468a98c48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8ba7fa82b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2D Sinusoidal signal generator GAN"
      ],
      "metadata": {
        "id": "E53BbwF2ngLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the Discriminator\n",
        "\n",
        "In PyTorch, the neural network models are represented by classes that inherit from nn.Module.\n",
        "\n",
        "The discriminator is a model with a two-dimensional input and a one-dimensional output. It’ll receive a sample from the real data or from the generator and will provide the probability that the sample belongs to the real training data.\n",
        "\n",
        "To build the model. First, you need to call super().__init__() to run .__init__() from nn.Module. Then the model id defined in a sequential way using nn.Sequential()"
      ],
      "metadata": {
        "id": "YAi3rv1UlTod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This discriminator is an MLP neural network \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            #The input is 28-dimensional, and the first hidden layer is composed \n",
        "            # of 256 neurons with ReLU activation.\n",
        "            nn.Linear(28, 256),\n",
        "            nn.ReLU(),\n",
        "            # use dropout after hidden layer to avoid overfitting.\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1),\n",
        "            # The output is composed of a single neuron with sigmoidal activation\n",
        "            # to represent a probability.\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "     # .forward() to describe how the output of the model is calculated\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "# instantiate a Discriminator object\n",
        "discriminator = Discriminator()"
      ],
      "metadata": {
        "id": "72rB6fQGlTDt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "\n",
        "In generative adversarial networks, the generator is the model that takes samples from a latent space as its input and generates data resembling the data in the training set. In this case, it’s a model with a two-dimensional input, which will receive random points (z₁, z₂), and a two-dimensional output that must provide (x̃₁, x̃₂) points resembling those from the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "KRcqwZflntcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 28),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "generator = Generator()"
      ],
      "metadata": {
        "id": "kD10TWE3npkj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way, the output will consist of a vector with two elements that can be any value ranging from negative infinity to infinity, which will represent (x̃₁, x̃₂)."
      ],
      "metadata": {
        "id": "qJwQdBpooHZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Training Data Sin(x)\n",
        "The training data is composed of pairs (x₁, x₂) so that x₂ consists of the value of the sine of x₁ for x₁ in the interval from 0 to 2π"
      ],
      "metadata": {
        "id": "QoYhq9F-i1cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load EB_ML python libraries\n",
        "# The following libraries are used in this notebook and should be installed in your local machine before running this notebook.\n",
        "# eb_colab_utils.py\n",
        "# eb_ml_battery_lib.py\n",
        "# eb_ml_utils.py\n",
        "\n",
        "# path to load external *.py files used in this notebook\n",
        "# Note: in Google Colab virtual machine you shoud copy the files in \"/content\" folder after BEFORE running this notebook's cell\n",
        "external_python_file_path=\"'/.'\"\n",
        "sys.path.append(external_python_file_path)\n",
        "\n",
        "\n",
        "from eb_ml_colab_utils import get_root_path,copy_model_to_google_drive\n",
        "from eb_ml_battery_lib import load_soc_dataset,generate_image_files_from_eis\n",
        "from eb_ml_utils import save_model_weights,build_data_loader,build_and_train_learner,score_model"
      ],
      "metadata": {
        "id": "4jJ1ntopcziK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#configuration dictionary\n",
        "config ={}\n",
        "\n",
        "# Root working folder (local or Google Drive)\n",
        "# config['ROOT_DIR'] = get_root_path(\"batterie\")\n",
        "config['ROOT_DIR'] = get_root_path(\"batterie\")  \n",
        "\n",
        "# Folder with dataset in CSV format\n",
        "#config['DATASETS_DIR'] = config['ROOT_DIR']+\"/datasets\"\n",
        "config['DATASETS_DIR'] = config['ROOT_DIR']+\"/datasets/EIS-vs-SOC-2022\"\n",
        "\n",
        "# List of SoC level into dataset\n",
        "#config['soc_list']=['100','090','080','070','060','050','040','030','020','010']\n",
        "config['soc_list']=['100','090','080','070','060','050','040','030','020','010']\n",
        "\n",
        "\n",
        "# Folder to store trained model\n",
        "#config['MODELS_DIR'] = config['ROOT_DIR']+\"/models\"\n",
        "config['MODELS_DIR'] = config['ROOT_DIR']+\"/models\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd5lis1Ec6HW",
        "outputId": "a08990f8-3d54-43f3-f27c-de122cad029b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on COLAB\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data acquition file to load from dateset folder\n",
        "# EIS Dataset https://data.mendeley.com/datasets/ch3sydbbrg/2\n",
        "frequency_list=[ 0.05, 0.1, 0.2, 0.4, 1, 2, 4, 10, 20, 40, 100, 200, 400, 1000]\n",
        "battery_list=[1,2,3,4,5,7,8,9,10,11,12,13] #6\n",
        "dataset,feature_col_names=load_soc_dataset(battery_list,config[\"soc_list\"],config['DATASETS_DIR'])"
      ],
      "metadata": {
        "id": "zAWvQQ-mc85P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soc_10_training=dataset.query('SOC== \"010\"')[feature_col_names]\n",
        "soc_10_training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "IzqxlXYTc_00",
        "outputId": "fca5be52-180d-48ff-d96b-70ae0e82cdfa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Z_f0                Z_f1                Z_f2  \\\n",
              "010  0.079771-0.005349j  0.077740-0.004683j  0.076173-0.003523j   \n",
              "010  0.078894-0.004942j  0.077379-0.003965j  0.076076-0.003162j   \n",
              "010  0.074923-0.004870j  0.073477-0.003759j  0.072532-0.002710j   \n",
              "010  0.074411-0.004880j  0.072787-0.003883j  0.071541-0.003067j   \n",
              "010  0.078197-0.004944j  0.076708-0.003921j  0.075516-0.002831j   \n",
              "010  0.073499-0.004352j  0.071975-0.003375j  0.071127-0.002276j   \n",
              "010  0.075002-0.004450j  0.073594-0.003316j  0.072728-0.002386j   \n",
              "010  0.079897-0.004582j  0.078391-0.003490j  0.077319-0.002565j   \n",
              "010  0.077036-0.004560j  0.075534-0.003552j  0.074379-0.002565j   \n",
              "010  0.097612-0.004647j  0.095967-0.003856j  0.094797-0.002671j   \n",
              "010  0.071875-0.003984j  0.070516-0.003260j  0.069583-0.002225j   \n",
              "010  0.074011-0.004292j  0.072539-0.003420j  0.071548-0.002388j   \n",
              "\n",
              "                   Z_f3                Z_f4                Z_f5  \\\n",
              "010  0.075399-0.003229j  0.073304-0.003548j  0.071919-0.004337j   \n",
              "010  0.075439-0.002577j  0.073901-0.002919j  0.072642-0.003744j   \n",
              "010  0.071758-0.002316j  0.070510-0.002377j  0.069565-0.003037j   \n",
              "010  0.070933-0.002429j  0.069589-0.002628j  0.068560-0.003292j   \n",
              "010  0.074972-0.002334j  0.073589-0.002613j  0.072889-0.003227j   \n",
              "010  0.070579-0.001976j  0.069628-0.001913j  0.069061-0.002316j   \n",
              "010  0.072421-0.002074j  0.071293-0.002048j  0.070492-0.002500j   \n",
              "010  0.076870-0.001969j  0.075815-0.002046j  0.075152-0.002592j   \n",
              "010  0.073773-0.002133j  0.072797-0.002270j  0.072098-0.002704j   \n",
              "010  0.094201-0.002246j  0.093193-0.002260j  0.092317-0.002724j   \n",
              "010  0.069102-0.001781j  0.068355-0.001730j  0.067728-0.002101j   \n",
              "010  0.071018-0.001970j  0.070191-0.001974j  0.069458-0.002370j   \n",
              "\n",
              "                   Z_f6                Z_f7                Z_f8  \\\n",
              "010  0.069978-0.004903j  0.066565-0.005511j  0.063821-0.005493j   \n",
              "010  0.070944-0.004596j  0.067612-0.005322j  0.064984-0.005585j   \n",
              "010  0.068364-0.003993j  0.065447-0.005366j  0.062517-0.006183j   \n",
              "010  0.067239-0.003997j  0.064362-0.005227j  0.061435-0.005945j   \n",
              "010  0.071461-0.004222j  0.068578-0.005554j  0.065428-0.006255j   \n",
              "010  0.068349-0.002973j  0.066075-0.004737j  0.063371-0.005850j   \n",
              "010  0.069626-0.003237j  0.067319-0.004893j  0.064682-0.005829j   \n",
              "010  0.074303-0.003259j  0.071812-0.004872j  0.069079-0.005790j   \n",
              "010  0.071250-0.003512j  0.068566-0.005268j  0.065698-0.006345j   \n",
              "010  0.091361-0.003504j  0.088793-0.004872j  0.086085-0.005751j   \n",
              "010  0.066984-0.002790j  0.065122-0.004238j  0.062783-0.005229j   \n",
              "010  0.068559-0.003093j  0.066522-0.004606j  0.063936-0.005608j   \n",
              "\n",
              "                   Z_f9               Z_f10               Z_f11  \\\n",
              "010  0.061411-0.005366j  0.057958-0.005174j  0.055633-0.004818j   \n",
              "010  0.062448-0.005382j  0.059086-0.005205j  0.056763-0.004722j   \n",
              "010  0.059429-0.006285j  0.055555-0.005599j  0.053129-0.004991j   \n",
              "010  0.058558-0.005913j  0.054922-0.005389j  0.052485-0.004896j   \n",
              "010  0.062342-0.006436j  0.058299-0.005866j  0.055709-0.005264j   \n",
              "010  0.060204-0.006134j  0.056277-0.005871j  0.053678-0.004992j   \n",
              "010  0.061710-0.006009j  0.057768-0.005385j  0.055370-0.004826j   \n",
              "010  0.065966-0.006023j  0.062122-0.005708j  0.059560-0.004794j   \n",
              "010  0.062095-0.006770j  0.057652-0.006236j  0.054922-0.005423j   \n",
              "010  0.083177-0.006003j  0.079377-0.005685j  0.076842-0.005000j   \n",
              "010  0.059880-0.005821j  0.056042-0.005589j  0.053541-0.004915j   \n",
              "010  0.060867-0.006227j  0.056886-0.005825j  0.054247-0.005141j   \n",
              "\n",
              "                  Z_f12               Z_f13  \n",
              "010  0.053745-0.004118j  0.051447-0.003001j  \n",
              "010  0.054563-0.004204j  0.052500-0.002999j  \n",
              "010  0.050902-0.004287j  0.048653-0.003007j  \n",
              "010  0.050414-0.004192j  0.048036-0.003069j  \n",
              "010  0.053496-0.004382j  0.051192-0.003051j  \n",
              "010  0.051553-0.004286j  0.049336-0.002925j  \n",
              "010  0.053423-0.003888j  0.051324-0.002779j  \n",
              "010  0.057592-0.003948j  0.055631-0.002843j  \n",
              "010  0.052739-0.004440j  0.050409-0.003261j  \n",
              "010  0.074681-0.004194j  0.072418-0.002935j  \n",
              "010  0.051409-0.004200j  0.049157-0.002987j  \n",
              "010  0.052139-0.004326j  0.049798-0.003033j  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d354e951-2518-4214-8364-261fe5f8c9a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Z_f0</th>\n",
              "      <th>Z_f1</th>\n",
              "      <th>Z_f2</th>\n",
              "      <th>Z_f3</th>\n",
              "      <th>Z_f4</th>\n",
              "      <th>Z_f5</th>\n",
              "      <th>Z_f6</th>\n",
              "      <th>Z_f7</th>\n",
              "      <th>Z_f8</th>\n",
              "      <th>Z_f9</th>\n",
              "      <th>Z_f10</th>\n",
              "      <th>Z_f11</th>\n",
              "      <th>Z_f12</th>\n",
              "      <th>Z_f13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.079771-0.005349j</td>\n",
              "      <td>0.077740-0.004683j</td>\n",
              "      <td>0.076173-0.003523j</td>\n",
              "      <td>0.075399-0.003229j</td>\n",
              "      <td>0.073304-0.003548j</td>\n",
              "      <td>0.071919-0.004337j</td>\n",
              "      <td>0.069978-0.004903j</td>\n",
              "      <td>0.066565-0.005511j</td>\n",
              "      <td>0.063821-0.005493j</td>\n",
              "      <td>0.061411-0.005366j</td>\n",
              "      <td>0.057958-0.005174j</td>\n",
              "      <td>0.055633-0.004818j</td>\n",
              "      <td>0.053745-0.004118j</td>\n",
              "      <td>0.051447-0.003001j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.078894-0.004942j</td>\n",
              "      <td>0.077379-0.003965j</td>\n",
              "      <td>0.076076-0.003162j</td>\n",
              "      <td>0.075439-0.002577j</td>\n",
              "      <td>0.073901-0.002919j</td>\n",
              "      <td>0.072642-0.003744j</td>\n",
              "      <td>0.070944-0.004596j</td>\n",
              "      <td>0.067612-0.005322j</td>\n",
              "      <td>0.064984-0.005585j</td>\n",
              "      <td>0.062448-0.005382j</td>\n",
              "      <td>0.059086-0.005205j</td>\n",
              "      <td>0.056763-0.004722j</td>\n",
              "      <td>0.054563-0.004204j</td>\n",
              "      <td>0.052500-0.002999j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.074923-0.004870j</td>\n",
              "      <td>0.073477-0.003759j</td>\n",
              "      <td>0.072532-0.002710j</td>\n",
              "      <td>0.071758-0.002316j</td>\n",
              "      <td>0.070510-0.002377j</td>\n",
              "      <td>0.069565-0.003037j</td>\n",
              "      <td>0.068364-0.003993j</td>\n",
              "      <td>0.065447-0.005366j</td>\n",
              "      <td>0.062517-0.006183j</td>\n",
              "      <td>0.059429-0.006285j</td>\n",
              "      <td>0.055555-0.005599j</td>\n",
              "      <td>0.053129-0.004991j</td>\n",
              "      <td>0.050902-0.004287j</td>\n",
              "      <td>0.048653-0.003007j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.074411-0.004880j</td>\n",
              "      <td>0.072787-0.003883j</td>\n",
              "      <td>0.071541-0.003067j</td>\n",
              "      <td>0.070933-0.002429j</td>\n",
              "      <td>0.069589-0.002628j</td>\n",
              "      <td>0.068560-0.003292j</td>\n",
              "      <td>0.067239-0.003997j</td>\n",
              "      <td>0.064362-0.005227j</td>\n",
              "      <td>0.061435-0.005945j</td>\n",
              "      <td>0.058558-0.005913j</td>\n",
              "      <td>0.054922-0.005389j</td>\n",
              "      <td>0.052485-0.004896j</td>\n",
              "      <td>0.050414-0.004192j</td>\n",
              "      <td>0.048036-0.003069j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.078197-0.004944j</td>\n",
              "      <td>0.076708-0.003921j</td>\n",
              "      <td>0.075516-0.002831j</td>\n",
              "      <td>0.074972-0.002334j</td>\n",
              "      <td>0.073589-0.002613j</td>\n",
              "      <td>0.072889-0.003227j</td>\n",
              "      <td>0.071461-0.004222j</td>\n",
              "      <td>0.068578-0.005554j</td>\n",
              "      <td>0.065428-0.006255j</td>\n",
              "      <td>0.062342-0.006436j</td>\n",
              "      <td>0.058299-0.005866j</td>\n",
              "      <td>0.055709-0.005264j</td>\n",
              "      <td>0.053496-0.004382j</td>\n",
              "      <td>0.051192-0.003051j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.073499-0.004352j</td>\n",
              "      <td>0.071975-0.003375j</td>\n",
              "      <td>0.071127-0.002276j</td>\n",
              "      <td>0.070579-0.001976j</td>\n",
              "      <td>0.069628-0.001913j</td>\n",
              "      <td>0.069061-0.002316j</td>\n",
              "      <td>0.068349-0.002973j</td>\n",
              "      <td>0.066075-0.004737j</td>\n",
              "      <td>0.063371-0.005850j</td>\n",
              "      <td>0.060204-0.006134j</td>\n",
              "      <td>0.056277-0.005871j</td>\n",
              "      <td>0.053678-0.004992j</td>\n",
              "      <td>0.051553-0.004286j</td>\n",
              "      <td>0.049336-0.002925j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.075002-0.004450j</td>\n",
              "      <td>0.073594-0.003316j</td>\n",
              "      <td>0.072728-0.002386j</td>\n",
              "      <td>0.072421-0.002074j</td>\n",
              "      <td>0.071293-0.002048j</td>\n",
              "      <td>0.070492-0.002500j</td>\n",
              "      <td>0.069626-0.003237j</td>\n",
              "      <td>0.067319-0.004893j</td>\n",
              "      <td>0.064682-0.005829j</td>\n",
              "      <td>0.061710-0.006009j</td>\n",
              "      <td>0.057768-0.005385j</td>\n",
              "      <td>0.055370-0.004826j</td>\n",
              "      <td>0.053423-0.003888j</td>\n",
              "      <td>0.051324-0.002779j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.079897-0.004582j</td>\n",
              "      <td>0.078391-0.003490j</td>\n",
              "      <td>0.077319-0.002565j</td>\n",
              "      <td>0.076870-0.001969j</td>\n",
              "      <td>0.075815-0.002046j</td>\n",
              "      <td>0.075152-0.002592j</td>\n",
              "      <td>0.074303-0.003259j</td>\n",
              "      <td>0.071812-0.004872j</td>\n",
              "      <td>0.069079-0.005790j</td>\n",
              "      <td>0.065966-0.006023j</td>\n",
              "      <td>0.062122-0.005708j</td>\n",
              "      <td>0.059560-0.004794j</td>\n",
              "      <td>0.057592-0.003948j</td>\n",
              "      <td>0.055631-0.002843j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.077036-0.004560j</td>\n",
              "      <td>0.075534-0.003552j</td>\n",
              "      <td>0.074379-0.002565j</td>\n",
              "      <td>0.073773-0.002133j</td>\n",
              "      <td>0.072797-0.002270j</td>\n",
              "      <td>0.072098-0.002704j</td>\n",
              "      <td>0.071250-0.003512j</td>\n",
              "      <td>0.068566-0.005268j</td>\n",
              "      <td>0.065698-0.006345j</td>\n",
              "      <td>0.062095-0.006770j</td>\n",
              "      <td>0.057652-0.006236j</td>\n",
              "      <td>0.054922-0.005423j</td>\n",
              "      <td>0.052739-0.004440j</td>\n",
              "      <td>0.050409-0.003261j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.097612-0.004647j</td>\n",
              "      <td>0.095967-0.003856j</td>\n",
              "      <td>0.094797-0.002671j</td>\n",
              "      <td>0.094201-0.002246j</td>\n",
              "      <td>0.093193-0.002260j</td>\n",
              "      <td>0.092317-0.002724j</td>\n",
              "      <td>0.091361-0.003504j</td>\n",
              "      <td>0.088793-0.004872j</td>\n",
              "      <td>0.086085-0.005751j</td>\n",
              "      <td>0.083177-0.006003j</td>\n",
              "      <td>0.079377-0.005685j</td>\n",
              "      <td>0.076842-0.005000j</td>\n",
              "      <td>0.074681-0.004194j</td>\n",
              "      <td>0.072418-0.002935j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.071875-0.003984j</td>\n",
              "      <td>0.070516-0.003260j</td>\n",
              "      <td>0.069583-0.002225j</td>\n",
              "      <td>0.069102-0.001781j</td>\n",
              "      <td>0.068355-0.001730j</td>\n",
              "      <td>0.067728-0.002101j</td>\n",
              "      <td>0.066984-0.002790j</td>\n",
              "      <td>0.065122-0.004238j</td>\n",
              "      <td>0.062783-0.005229j</td>\n",
              "      <td>0.059880-0.005821j</td>\n",
              "      <td>0.056042-0.005589j</td>\n",
              "      <td>0.053541-0.004915j</td>\n",
              "      <td>0.051409-0.004200j</td>\n",
              "      <td>0.049157-0.002987j</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>010</th>\n",
              "      <td>0.074011-0.004292j</td>\n",
              "      <td>0.072539-0.003420j</td>\n",
              "      <td>0.071548-0.002388j</td>\n",
              "      <td>0.071018-0.001970j</td>\n",
              "      <td>0.070191-0.001974j</td>\n",
              "      <td>0.069458-0.002370j</td>\n",
              "      <td>0.068559-0.003093j</td>\n",
              "      <td>0.066522-0.004606j</td>\n",
              "      <td>0.063936-0.005608j</td>\n",
              "      <td>0.060867-0.006227j</td>\n",
              "      <td>0.056886-0.005825j</td>\n",
              "      <td>0.054247-0.005141j</td>\n",
              "      <td>0.052139-0.004326j</td>\n",
              "      <td>0.049798-0.003033j</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d354e951-2518-4214-8364-261fe5f8c9a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d354e951-2518-4214-8364-261fe5f8c9a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d354e951-2518-4214-8364-261fe5f8c9a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_EIS_num = len(soc_10_training)\n",
        "n_frequency =len(frequency_list)\n",
        "train_data_length = 100\n",
        "\n",
        "# initialize train_data, a tensor with dimensions of <train_data_length> rows and 3 columns, all containing zeros. \n",
        "train_data = torch.zeros((train_data_length,n_frequency*2))"
      ],
      "metadata": {
        "id": "39qrEqlRdELE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import mod\n",
        "from torch import functional\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "for i  in range(0,train_data_length):\n",
        "  row_index= i % total_EIS_num\n",
        "  print(\"Row index: \"+str(row_index))\n",
        "  # the second column of the tensor  ReZ(f) , tird column Img Z(f)\n",
        "  train_data[i] = torch.cat((functional.Tensor(np.real(soc_10_training))[row_index],functional.Tensor(np.imag(soc_10_training))[row_index]))"
      ],
      "metadata": {
        "id": "pmk50033dSat",
        "outputId": "4ad06691-7278-414c-96db-5a83093f34fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n",
            "Row index: 4\n",
            "Row index: 5\n",
            "Row index: 6\n",
            "Row index: 7\n",
            "Row index: 8\n",
            "Row index: 9\n",
            "Row index: 10\n",
            "Row index: 11\n",
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n",
            "Row index: 4\n",
            "Row index: 5\n",
            "Row index: 6\n",
            "Row index: 7\n",
            "Row index: 8\n",
            "Row index: 9\n",
            "Row index: 10\n",
            "Row index: 11\n",
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n",
            "Row index: 4\n",
            "Row index: 5\n",
            "Row index: 6\n",
            "Row index: 7\n",
            "Row index: 8\n",
            "Row index: 9\n",
            "Row index: 10\n",
            "Row index: 11\n",
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n",
            "Row index: 4\n",
            "Row index: 5\n",
            "Row index: 6\n",
            "Row index: 7\n",
            "Row index: 8\n",
            "Row index: 9\n",
            "Row index: 10\n",
            "Row index: 11\n",
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n",
            "Row index: 4\n",
            "Row index: 5\n",
            "Row index: 6\n",
            "Row index: 7\n",
            "Row index: 8\n",
            "Row index: 9\n",
            "Row index: 10\n",
            "Row index: 11\n",
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n",
            "Row index: 4\n",
            "Row index: 5\n",
            "Row index: 6\n",
            "Row index: 7\n",
            "Row index: 8\n",
            "Row index: 9\n",
            "Row index: 10\n",
            "Row index: 11\n",
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n",
            "Row index: 4\n",
            "Row index: 5\n",
            "Row index: 6\n",
            "Row index: 7\n",
            "Row index: 8\n",
            "Row index: 9\n",
            "Row index: 10\n",
            "Row index: 11\n",
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n",
            "Row index: 4\n",
            "Row index: 5\n",
            "Row index: 6\n",
            "Row index: 7\n",
            "Row index: 8\n",
            "Row index: 9\n",
            "Row index: 10\n",
            "Row index: 11\n",
            "Row index: 0\n",
            "Row index: 1\n",
            "Row index: 2\n",
            "Row index: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init tensor of labels, which are required by PyTorch’s data loader. \n",
        "# Since GANs make use of unsupervised learning techniques, the labels can be anything.\n",
        "train_labels = torch.zeros(train_data_length)\n",
        "\n",
        "# create train_set as a list of tuples, with each row of train_data and train_labels represented in each tuple \n",
        "# this is the format expected by PyTorch’s data loader.\n",
        "train_set = [\n",
        "    (train_data[i], train_labels[i]) for i in range(train_data_length)\n",
        "]"
      ],
      "metadata": {
        "id": "By7gUloSicB6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88sT8gt5jEKQ",
        "outputId": "ceb7e529-38f8-44c4-f3f9-2f35df9cc227"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]),\n",
              "  tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0782,  0.0767,  0.0755,  0.0750,  0.0736,  0.0729,  0.0715,  0.0686,\n",
              "           0.0654,  0.0623,  0.0583,  0.0557,  0.0535,  0.0512, -0.0049, -0.0039,\n",
              "          -0.0028, -0.0023, -0.0026, -0.0032, -0.0042, -0.0056, -0.0063, -0.0064,\n",
              "          -0.0059, -0.0053, -0.0044, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0735,  0.0720,  0.0711,  0.0706,  0.0696,  0.0691,  0.0683,  0.0661,\n",
              "           0.0634,  0.0602,  0.0563,  0.0537,  0.0516,  0.0493, -0.0044, -0.0034,\n",
              "          -0.0023, -0.0020, -0.0019, -0.0023, -0.0030, -0.0047, -0.0059, -0.0061,\n",
              "          -0.0059, -0.0050, -0.0043, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0750,  0.0736,  0.0727,  0.0724,  0.0713,  0.0705,  0.0696,  0.0673,\n",
              "           0.0647,  0.0617,  0.0578,  0.0554,  0.0534,  0.0513, -0.0045, -0.0033,\n",
              "          -0.0024, -0.0021, -0.0020, -0.0025, -0.0032, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0054, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0799,  0.0784,  0.0773,  0.0769,  0.0758,  0.0752,  0.0743,  0.0718,\n",
              "           0.0691,  0.0660,  0.0621,  0.0596,  0.0576,  0.0556, -0.0046, -0.0035,\n",
              "          -0.0026, -0.0020, -0.0020, -0.0026, -0.0033, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0770,  0.0755,  0.0744,  0.0738,  0.0728,  0.0721,  0.0712,  0.0686,\n",
              "           0.0657,  0.0621,  0.0577,  0.0549,  0.0527,  0.0504, -0.0046, -0.0036,\n",
              "          -0.0026, -0.0021, -0.0023, -0.0027, -0.0035, -0.0053, -0.0063, -0.0068,\n",
              "          -0.0062, -0.0054, -0.0044, -0.0033]), tensor(0.)),\n",
              " (tensor([ 0.0976,  0.0960,  0.0948,  0.0942,  0.0932,  0.0923,  0.0914,  0.0888,\n",
              "           0.0861,  0.0832,  0.0794,  0.0768,  0.0747,  0.0724, -0.0046, -0.0039,\n",
              "          -0.0027, -0.0022, -0.0023, -0.0027, -0.0035, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0050, -0.0042, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0719,  0.0705,  0.0696,  0.0691,  0.0684,  0.0677,  0.0670,  0.0651,\n",
              "           0.0628,  0.0599,  0.0560,  0.0535,  0.0514,  0.0492, -0.0040, -0.0033,\n",
              "          -0.0022, -0.0018, -0.0017, -0.0021, -0.0028, -0.0042, -0.0052, -0.0058,\n",
              "          -0.0056, -0.0049, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0740,  0.0725,  0.0715,  0.0710,  0.0702,  0.0695,  0.0686,  0.0665,\n",
              "           0.0639,  0.0609,  0.0569,  0.0542,  0.0521,  0.0498, -0.0043, -0.0034,\n",
              "          -0.0024, -0.0020, -0.0020, -0.0024, -0.0031, -0.0046, -0.0056, -0.0062,\n",
              "          -0.0058, -0.0051, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0782,  0.0767,  0.0755,  0.0750,  0.0736,  0.0729,  0.0715,  0.0686,\n",
              "           0.0654,  0.0623,  0.0583,  0.0557,  0.0535,  0.0512, -0.0049, -0.0039,\n",
              "          -0.0028, -0.0023, -0.0026, -0.0032, -0.0042, -0.0056, -0.0063, -0.0064,\n",
              "          -0.0059, -0.0053, -0.0044, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0735,  0.0720,  0.0711,  0.0706,  0.0696,  0.0691,  0.0683,  0.0661,\n",
              "           0.0634,  0.0602,  0.0563,  0.0537,  0.0516,  0.0493, -0.0044, -0.0034,\n",
              "          -0.0023, -0.0020, -0.0019, -0.0023, -0.0030, -0.0047, -0.0059, -0.0061,\n",
              "          -0.0059, -0.0050, -0.0043, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0750,  0.0736,  0.0727,  0.0724,  0.0713,  0.0705,  0.0696,  0.0673,\n",
              "           0.0647,  0.0617,  0.0578,  0.0554,  0.0534,  0.0513, -0.0045, -0.0033,\n",
              "          -0.0024, -0.0021, -0.0020, -0.0025, -0.0032, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0054, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0799,  0.0784,  0.0773,  0.0769,  0.0758,  0.0752,  0.0743,  0.0718,\n",
              "           0.0691,  0.0660,  0.0621,  0.0596,  0.0576,  0.0556, -0.0046, -0.0035,\n",
              "          -0.0026, -0.0020, -0.0020, -0.0026, -0.0033, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0770,  0.0755,  0.0744,  0.0738,  0.0728,  0.0721,  0.0712,  0.0686,\n",
              "           0.0657,  0.0621,  0.0577,  0.0549,  0.0527,  0.0504, -0.0046, -0.0036,\n",
              "          -0.0026, -0.0021, -0.0023, -0.0027, -0.0035, -0.0053, -0.0063, -0.0068,\n",
              "          -0.0062, -0.0054, -0.0044, -0.0033]), tensor(0.)),\n",
              " (tensor([ 0.0976,  0.0960,  0.0948,  0.0942,  0.0932,  0.0923,  0.0914,  0.0888,\n",
              "           0.0861,  0.0832,  0.0794,  0.0768,  0.0747,  0.0724, -0.0046, -0.0039,\n",
              "          -0.0027, -0.0022, -0.0023, -0.0027, -0.0035, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0050, -0.0042, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0719,  0.0705,  0.0696,  0.0691,  0.0684,  0.0677,  0.0670,  0.0651,\n",
              "           0.0628,  0.0599,  0.0560,  0.0535,  0.0514,  0.0492, -0.0040, -0.0033,\n",
              "          -0.0022, -0.0018, -0.0017, -0.0021, -0.0028, -0.0042, -0.0052, -0.0058,\n",
              "          -0.0056, -0.0049, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0740,  0.0725,  0.0715,  0.0710,  0.0702,  0.0695,  0.0686,  0.0665,\n",
              "           0.0639,  0.0609,  0.0569,  0.0542,  0.0521,  0.0498, -0.0043, -0.0034,\n",
              "          -0.0024, -0.0020, -0.0020, -0.0024, -0.0031, -0.0046, -0.0056, -0.0062,\n",
              "          -0.0058, -0.0051, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0782,  0.0767,  0.0755,  0.0750,  0.0736,  0.0729,  0.0715,  0.0686,\n",
              "           0.0654,  0.0623,  0.0583,  0.0557,  0.0535,  0.0512, -0.0049, -0.0039,\n",
              "          -0.0028, -0.0023, -0.0026, -0.0032, -0.0042, -0.0056, -0.0063, -0.0064,\n",
              "          -0.0059, -0.0053, -0.0044, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0735,  0.0720,  0.0711,  0.0706,  0.0696,  0.0691,  0.0683,  0.0661,\n",
              "           0.0634,  0.0602,  0.0563,  0.0537,  0.0516,  0.0493, -0.0044, -0.0034,\n",
              "          -0.0023, -0.0020, -0.0019, -0.0023, -0.0030, -0.0047, -0.0059, -0.0061,\n",
              "          -0.0059, -0.0050, -0.0043, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0750,  0.0736,  0.0727,  0.0724,  0.0713,  0.0705,  0.0696,  0.0673,\n",
              "           0.0647,  0.0617,  0.0578,  0.0554,  0.0534,  0.0513, -0.0045, -0.0033,\n",
              "          -0.0024, -0.0021, -0.0020, -0.0025, -0.0032, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0054, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0799,  0.0784,  0.0773,  0.0769,  0.0758,  0.0752,  0.0743,  0.0718,\n",
              "           0.0691,  0.0660,  0.0621,  0.0596,  0.0576,  0.0556, -0.0046, -0.0035,\n",
              "          -0.0026, -0.0020, -0.0020, -0.0026, -0.0033, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0770,  0.0755,  0.0744,  0.0738,  0.0728,  0.0721,  0.0712,  0.0686,\n",
              "           0.0657,  0.0621,  0.0577,  0.0549,  0.0527,  0.0504, -0.0046, -0.0036,\n",
              "          -0.0026, -0.0021, -0.0023, -0.0027, -0.0035, -0.0053, -0.0063, -0.0068,\n",
              "          -0.0062, -0.0054, -0.0044, -0.0033]), tensor(0.)),\n",
              " (tensor([ 0.0976,  0.0960,  0.0948,  0.0942,  0.0932,  0.0923,  0.0914,  0.0888,\n",
              "           0.0861,  0.0832,  0.0794,  0.0768,  0.0747,  0.0724, -0.0046, -0.0039,\n",
              "          -0.0027, -0.0022, -0.0023, -0.0027, -0.0035, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0050, -0.0042, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0719,  0.0705,  0.0696,  0.0691,  0.0684,  0.0677,  0.0670,  0.0651,\n",
              "           0.0628,  0.0599,  0.0560,  0.0535,  0.0514,  0.0492, -0.0040, -0.0033,\n",
              "          -0.0022, -0.0018, -0.0017, -0.0021, -0.0028, -0.0042, -0.0052, -0.0058,\n",
              "          -0.0056, -0.0049, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0740,  0.0725,  0.0715,  0.0710,  0.0702,  0.0695,  0.0686,  0.0665,\n",
              "           0.0639,  0.0609,  0.0569,  0.0542,  0.0521,  0.0498, -0.0043, -0.0034,\n",
              "          -0.0024, -0.0020, -0.0020, -0.0024, -0.0031, -0.0046, -0.0056, -0.0062,\n",
              "          -0.0058, -0.0051, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0782,  0.0767,  0.0755,  0.0750,  0.0736,  0.0729,  0.0715,  0.0686,\n",
              "           0.0654,  0.0623,  0.0583,  0.0557,  0.0535,  0.0512, -0.0049, -0.0039,\n",
              "          -0.0028, -0.0023, -0.0026, -0.0032, -0.0042, -0.0056, -0.0063, -0.0064,\n",
              "          -0.0059, -0.0053, -0.0044, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0735,  0.0720,  0.0711,  0.0706,  0.0696,  0.0691,  0.0683,  0.0661,\n",
              "           0.0634,  0.0602,  0.0563,  0.0537,  0.0516,  0.0493, -0.0044, -0.0034,\n",
              "          -0.0023, -0.0020, -0.0019, -0.0023, -0.0030, -0.0047, -0.0059, -0.0061,\n",
              "          -0.0059, -0.0050, -0.0043, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0750,  0.0736,  0.0727,  0.0724,  0.0713,  0.0705,  0.0696,  0.0673,\n",
              "           0.0647,  0.0617,  0.0578,  0.0554,  0.0534,  0.0513, -0.0045, -0.0033,\n",
              "          -0.0024, -0.0021, -0.0020, -0.0025, -0.0032, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0054, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0799,  0.0784,  0.0773,  0.0769,  0.0758,  0.0752,  0.0743,  0.0718,\n",
              "           0.0691,  0.0660,  0.0621,  0.0596,  0.0576,  0.0556, -0.0046, -0.0035,\n",
              "          -0.0026, -0.0020, -0.0020, -0.0026, -0.0033, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0770,  0.0755,  0.0744,  0.0738,  0.0728,  0.0721,  0.0712,  0.0686,\n",
              "           0.0657,  0.0621,  0.0577,  0.0549,  0.0527,  0.0504, -0.0046, -0.0036,\n",
              "          -0.0026, -0.0021, -0.0023, -0.0027, -0.0035, -0.0053, -0.0063, -0.0068,\n",
              "          -0.0062, -0.0054, -0.0044, -0.0033]), tensor(0.)),\n",
              " (tensor([ 0.0976,  0.0960,  0.0948,  0.0942,  0.0932,  0.0923,  0.0914,  0.0888,\n",
              "           0.0861,  0.0832,  0.0794,  0.0768,  0.0747,  0.0724, -0.0046, -0.0039,\n",
              "          -0.0027, -0.0022, -0.0023, -0.0027, -0.0035, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0050, -0.0042, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0719,  0.0705,  0.0696,  0.0691,  0.0684,  0.0677,  0.0670,  0.0651,\n",
              "           0.0628,  0.0599,  0.0560,  0.0535,  0.0514,  0.0492, -0.0040, -0.0033,\n",
              "          -0.0022, -0.0018, -0.0017, -0.0021, -0.0028, -0.0042, -0.0052, -0.0058,\n",
              "          -0.0056, -0.0049, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0740,  0.0725,  0.0715,  0.0710,  0.0702,  0.0695,  0.0686,  0.0665,\n",
              "           0.0639,  0.0609,  0.0569,  0.0542,  0.0521,  0.0498, -0.0043, -0.0034,\n",
              "          -0.0024, -0.0020, -0.0020, -0.0024, -0.0031, -0.0046, -0.0056, -0.0062,\n",
              "          -0.0058, -0.0051, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0782,  0.0767,  0.0755,  0.0750,  0.0736,  0.0729,  0.0715,  0.0686,\n",
              "           0.0654,  0.0623,  0.0583,  0.0557,  0.0535,  0.0512, -0.0049, -0.0039,\n",
              "          -0.0028, -0.0023, -0.0026, -0.0032, -0.0042, -0.0056, -0.0063, -0.0064,\n",
              "          -0.0059, -0.0053, -0.0044, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0735,  0.0720,  0.0711,  0.0706,  0.0696,  0.0691,  0.0683,  0.0661,\n",
              "           0.0634,  0.0602,  0.0563,  0.0537,  0.0516,  0.0493, -0.0044, -0.0034,\n",
              "          -0.0023, -0.0020, -0.0019, -0.0023, -0.0030, -0.0047, -0.0059, -0.0061,\n",
              "          -0.0059, -0.0050, -0.0043, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0750,  0.0736,  0.0727,  0.0724,  0.0713,  0.0705,  0.0696,  0.0673,\n",
              "           0.0647,  0.0617,  0.0578,  0.0554,  0.0534,  0.0513, -0.0045, -0.0033,\n",
              "          -0.0024, -0.0021, -0.0020, -0.0025, -0.0032, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0054, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0799,  0.0784,  0.0773,  0.0769,  0.0758,  0.0752,  0.0743,  0.0718,\n",
              "           0.0691,  0.0660,  0.0621,  0.0596,  0.0576,  0.0556, -0.0046, -0.0035,\n",
              "          -0.0026, -0.0020, -0.0020, -0.0026, -0.0033, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0770,  0.0755,  0.0744,  0.0738,  0.0728,  0.0721,  0.0712,  0.0686,\n",
              "           0.0657,  0.0621,  0.0577,  0.0549,  0.0527,  0.0504, -0.0046, -0.0036,\n",
              "          -0.0026, -0.0021, -0.0023, -0.0027, -0.0035, -0.0053, -0.0063, -0.0068,\n",
              "          -0.0062, -0.0054, -0.0044, -0.0033]), tensor(0.)),\n",
              " (tensor([ 0.0976,  0.0960,  0.0948,  0.0942,  0.0932,  0.0923,  0.0914,  0.0888,\n",
              "           0.0861,  0.0832,  0.0794,  0.0768,  0.0747,  0.0724, -0.0046, -0.0039,\n",
              "          -0.0027, -0.0022, -0.0023, -0.0027, -0.0035, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0050, -0.0042, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0719,  0.0705,  0.0696,  0.0691,  0.0684,  0.0677,  0.0670,  0.0651,\n",
              "           0.0628,  0.0599,  0.0560,  0.0535,  0.0514,  0.0492, -0.0040, -0.0033,\n",
              "          -0.0022, -0.0018, -0.0017, -0.0021, -0.0028, -0.0042, -0.0052, -0.0058,\n",
              "          -0.0056, -0.0049, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0740,  0.0725,  0.0715,  0.0710,  0.0702,  0.0695,  0.0686,  0.0665,\n",
              "           0.0639,  0.0609,  0.0569,  0.0542,  0.0521,  0.0498, -0.0043, -0.0034,\n",
              "          -0.0024, -0.0020, -0.0020, -0.0024, -0.0031, -0.0046, -0.0056, -0.0062,\n",
              "          -0.0058, -0.0051, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0782,  0.0767,  0.0755,  0.0750,  0.0736,  0.0729,  0.0715,  0.0686,\n",
              "           0.0654,  0.0623,  0.0583,  0.0557,  0.0535,  0.0512, -0.0049, -0.0039,\n",
              "          -0.0028, -0.0023, -0.0026, -0.0032, -0.0042, -0.0056, -0.0063, -0.0064,\n",
              "          -0.0059, -0.0053, -0.0044, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0735,  0.0720,  0.0711,  0.0706,  0.0696,  0.0691,  0.0683,  0.0661,\n",
              "           0.0634,  0.0602,  0.0563,  0.0537,  0.0516,  0.0493, -0.0044, -0.0034,\n",
              "          -0.0023, -0.0020, -0.0019, -0.0023, -0.0030, -0.0047, -0.0059, -0.0061,\n",
              "          -0.0059, -0.0050, -0.0043, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0750,  0.0736,  0.0727,  0.0724,  0.0713,  0.0705,  0.0696,  0.0673,\n",
              "           0.0647,  0.0617,  0.0578,  0.0554,  0.0534,  0.0513, -0.0045, -0.0033,\n",
              "          -0.0024, -0.0021, -0.0020, -0.0025, -0.0032, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0054, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0799,  0.0784,  0.0773,  0.0769,  0.0758,  0.0752,  0.0743,  0.0718,\n",
              "           0.0691,  0.0660,  0.0621,  0.0596,  0.0576,  0.0556, -0.0046, -0.0035,\n",
              "          -0.0026, -0.0020, -0.0020, -0.0026, -0.0033, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0770,  0.0755,  0.0744,  0.0738,  0.0728,  0.0721,  0.0712,  0.0686,\n",
              "           0.0657,  0.0621,  0.0577,  0.0549,  0.0527,  0.0504, -0.0046, -0.0036,\n",
              "          -0.0026, -0.0021, -0.0023, -0.0027, -0.0035, -0.0053, -0.0063, -0.0068,\n",
              "          -0.0062, -0.0054, -0.0044, -0.0033]), tensor(0.)),\n",
              " (tensor([ 0.0976,  0.0960,  0.0948,  0.0942,  0.0932,  0.0923,  0.0914,  0.0888,\n",
              "           0.0861,  0.0832,  0.0794,  0.0768,  0.0747,  0.0724, -0.0046, -0.0039,\n",
              "          -0.0027, -0.0022, -0.0023, -0.0027, -0.0035, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0050, -0.0042, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0719,  0.0705,  0.0696,  0.0691,  0.0684,  0.0677,  0.0670,  0.0651,\n",
              "           0.0628,  0.0599,  0.0560,  0.0535,  0.0514,  0.0492, -0.0040, -0.0033,\n",
              "          -0.0022, -0.0018, -0.0017, -0.0021, -0.0028, -0.0042, -0.0052, -0.0058,\n",
              "          -0.0056, -0.0049, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0740,  0.0725,  0.0715,  0.0710,  0.0702,  0.0695,  0.0686,  0.0665,\n",
              "           0.0639,  0.0609,  0.0569,  0.0542,  0.0521,  0.0498, -0.0043, -0.0034,\n",
              "          -0.0024, -0.0020, -0.0020, -0.0024, -0.0031, -0.0046, -0.0056, -0.0062,\n",
              "          -0.0058, -0.0051, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0782,  0.0767,  0.0755,  0.0750,  0.0736,  0.0729,  0.0715,  0.0686,\n",
              "           0.0654,  0.0623,  0.0583,  0.0557,  0.0535,  0.0512, -0.0049, -0.0039,\n",
              "          -0.0028, -0.0023, -0.0026, -0.0032, -0.0042, -0.0056, -0.0063, -0.0064,\n",
              "          -0.0059, -0.0053, -0.0044, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0735,  0.0720,  0.0711,  0.0706,  0.0696,  0.0691,  0.0683,  0.0661,\n",
              "           0.0634,  0.0602,  0.0563,  0.0537,  0.0516,  0.0493, -0.0044, -0.0034,\n",
              "          -0.0023, -0.0020, -0.0019, -0.0023, -0.0030, -0.0047, -0.0059, -0.0061,\n",
              "          -0.0059, -0.0050, -0.0043, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0750,  0.0736,  0.0727,  0.0724,  0.0713,  0.0705,  0.0696,  0.0673,\n",
              "           0.0647,  0.0617,  0.0578,  0.0554,  0.0534,  0.0513, -0.0045, -0.0033,\n",
              "          -0.0024, -0.0021, -0.0020, -0.0025, -0.0032, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0054, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0799,  0.0784,  0.0773,  0.0769,  0.0758,  0.0752,  0.0743,  0.0718,\n",
              "           0.0691,  0.0660,  0.0621,  0.0596,  0.0576,  0.0556, -0.0046, -0.0035,\n",
              "          -0.0026, -0.0020, -0.0020, -0.0026, -0.0033, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0770,  0.0755,  0.0744,  0.0738,  0.0728,  0.0721,  0.0712,  0.0686,\n",
              "           0.0657,  0.0621,  0.0577,  0.0549,  0.0527,  0.0504, -0.0046, -0.0036,\n",
              "          -0.0026, -0.0021, -0.0023, -0.0027, -0.0035, -0.0053, -0.0063, -0.0068,\n",
              "          -0.0062, -0.0054, -0.0044, -0.0033]), tensor(0.)),\n",
              " (tensor([ 0.0976,  0.0960,  0.0948,  0.0942,  0.0932,  0.0923,  0.0914,  0.0888,\n",
              "           0.0861,  0.0832,  0.0794,  0.0768,  0.0747,  0.0724, -0.0046, -0.0039,\n",
              "          -0.0027, -0.0022, -0.0023, -0.0027, -0.0035, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0050, -0.0042, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0719,  0.0705,  0.0696,  0.0691,  0.0684,  0.0677,  0.0670,  0.0651,\n",
              "           0.0628,  0.0599,  0.0560,  0.0535,  0.0514,  0.0492, -0.0040, -0.0033,\n",
              "          -0.0022, -0.0018, -0.0017, -0.0021, -0.0028, -0.0042, -0.0052, -0.0058,\n",
              "          -0.0056, -0.0049, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0740,  0.0725,  0.0715,  0.0710,  0.0702,  0.0695,  0.0686,  0.0665,\n",
              "           0.0639,  0.0609,  0.0569,  0.0542,  0.0521,  0.0498, -0.0043, -0.0034,\n",
              "          -0.0024, -0.0020, -0.0020, -0.0024, -0.0031, -0.0046, -0.0056, -0.0062,\n",
              "          -0.0058, -0.0051, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0782,  0.0767,  0.0755,  0.0750,  0.0736,  0.0729,  0.0715,  0.0686,\n",
              "           0.0654,  0.0623,  0.0583,  0.0557,  0.0535,  0.0512, -0.0049, -0.0039,\n",
              "          -0.0028, -0.0023, -0.0026, -0.0032, -0.0042, -0.0056, -0.0063, -0.0064,\n",
              "          -0.0059, -0.0053, -0.0044, -0.0031]), tensor(0.)),\n",
              " (tensor([ 0.0735,  0.0720,  0.0711,  0.0706,  0.0696,  0.0691,  0.0683,  0.0661,\n",
              "           0.0634,  0.0602,  0.0563,  0.0537,  0.0516,  0.0493, -0.0044, -0.0034,\n",
              "          -0.0023, -0.0020, -0.0019, -0.0023, -0.0030, -0.0047, -0.0059, -0.0061,\n",
              "          -0.0059, -0.0050, -0.0043, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0750,  0.0736,  0.0727,  0.0724,  0.0713,  0.0705,  0.0696,  0.0673,\n",
              "           0.0647,  0.0617,  0.0578,  0.0554,  0.0534,  0.0513, -0.0045, -0.0033,\n",
              "          -0.0024, -0.0021, -0.0020, -0.0025, -0.0032, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0054, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0799,  0.0784,  0.0773,  0.0769,  0.0758,  0.0752,  0.0743,  0.0718,\n",
              "           0.0691,  0.0660,  0.0621,  0.0596,  0.0576,  0.0556, -0.0046, -0.0035,\n",
              "          -0.0026, -0.0020, -0.0020, -0.0026, -0.0033, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0048, -0.0039, -0.0028]), tensor(0.)),\n",
              " (tensor([ 0.0770,  0.0755,  0.0744,  0.0738,  0.0728,  0.0721,  0.0712,  0.0686,\n",
              "           0.0657,  0.0621,  0.0577,  0.0549,  0.0527,  0.0504, -0.0046, -0.0036,\n",
              "          -0.0026, -0.0021, -0.0023, -0.0027, -0.0035, -0.0053, -0.0063, -0.0068,\n",
              "          -0.0062, -0.0054, -0.0044, -0.0033]), tensor(0.)),\n",
              " (tensor([ 0.0976,  0.0960,  0.0948,  0.0942,  0.0932,  0.0923,  0.0914,  0.0888,\n",
              "           0.0861,  0.0832,  0.0794,  0.0768,  0.0747,  0.0724, -0.0046, -0.0039,\n",
              "          -0.0027, -0.0022, -0.0023, -0.0027, -0.0035, -0.0049, -0.0058, -0.0060,\n",
              "          -0.0057, -0.0050, -0.0042, -0.0029]), tensor(0.)),\n",
              " (tensor([ 0.0719,  0.0705,  0.0696,  0.0691,  0.0684,  0.0677,  0.0670,  0.0651,\n",
              "           0.0628,  0.0599,  0.0560,  0.0535,  0.0514,  0.0492, -0.0040, -0.0033,\n",
              "          -0.0022, -0.0018, -0.0017, -0.0021, -0.0028, -0.0042, -0.0052, -0.0058,\n",
              "          -0.0056, -0.0049, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0740,  0.0725,  0.0715,  0.0710,  0.0702,  0.0695,  0.0686,  0.0665,\n",
              "           0.0639,  0.0609,  0.0569,  0.0542,  0.0521,  0.0498, -0.0043, -0.0034,\n",
              "          -0.0024, -0.0020, -0.0020, -0.0024, -0.0031, -0.0046, -0.0056, -0.0062,\n",
              "          -0.0058, -0.0051, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0798,  0.0777,  0.0762,  0.0754,  0.0733,  0.0719,  0.0700,  0.0666,\n",
              "           0.0638,  0.0614,  0.0580,  0.0556,  0.0537,  0.0514, -0.0053, -0.0047,\n",
              "          -0.0035, -0.0032, -0.0035, -0.0043, -0.0049, -0.0055, -0.0055, -0.0054,\n",
              "          -0.0052, -0.0048, -0.0041, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0789,  0.0774,  0.0761,  0.0754,  0.0739,  0.0726,  0.0709,  0.0676,\n",
              "           0.0650,  0.0624,  0.0591,  0.0568,  0.0546,  0.0525, -0.0049, -0.0040,\n",
              "          -0.0032, -0.0026, -0.0029, -0.0037, -0.0046, -0.0053, -0.0056, -0.0054,\n",
              "          -0.0052, -0.0047, -0.0042, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0749,  0.0735,  0.0725,  0.0718,  0.0705,  0.0696,  0.0684,  0.0654,\n",
              "           0.0625,  0.0594,  0.0556,  0.0531,  0.0509,  0.0487, -0.0049, -0.0038,\n",
              "          -0.0027, -0.0023, -0.0024, -0.0030, -0.0040, -0.0054, -0.0062, -0.0063,\n",
              "          -0.0056, -0.0050, -0.0043, -0.0030]), tensor(0.)),\n",
              " (tensor([ 0.0744,  0.0728,  0.0715,  0.0709,  0.0696,  0.0686,  0.0672,  0.0644,\n",
              "           0.0614,  0.0586,  0.0549,  0.0525,  0.0504,  0.0480, -0.0049, -0.0039,\n",
              "          -0.0031, -0.0024, -0.0026, -0.0033, -0.0040, -0.0052, -0.0059, -0.0059,\n",
              "          -0.0054, -0.0049, -0.0042, -0.0031]), tensor(0.))]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(frequency_list, train_data[1,0:14], \".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "1fYK2Oi-jCVu",
        "outputId": "8836cb26-f484-4fbf-e2b5-92f76e5ee806"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8b8cdc1c10>]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(frequency_list, train_data[1,14:28], \".\")"
      ],
      "metadata": {
        "id": "ge6r7sg4bf0Z",
        "outputId": "9dc17ccf-a7ee-4a54-8b05-1a980b520fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8b8cd32550>]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_data[1,0:14], train_data[1,14:28], \"o-\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "5-2HmBR0eEtQ",
        "outputId": "19c39e20-1486-420e-93bd-b946d64f2006"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8b8cca2450>]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "TLc9Tt30oMqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch data loader called train_loader.\n",
        "# Data Loader will shuffle the data from train_set and return batches of <batch_size> samples \n",
        "# to be used to train the neural networks.\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, drop_last=True, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "yeBmjp3WkwQB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "num_epochs = 1000\n",
        "\n",
        "# The binary cross-entropy function is a suitable loss function for training the discriminator \n",
        "# because it considers a binary classification task. \n",
        "# It’s also suitable for training the generator since it feeds its output to the discriminator,\n",
        "# which provides a binary observable output.\n",
        "loss_function = nn.BCELoss()"
      ],
      "metadata": {
        "id": "KWJp-AtaoLYz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch implements various weight update rules for model training in torch.optim. You’ll use the Adam algorithm to train the discriminator and generator models."
      ],
      "metadata": {
        "id": "OoeUu-71o0QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "fnRht8oRor66"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop in which training samples are fed to the models, and their weights are updated to minimize the loss function.\n",
        "\n",
        "As is generally done for all neural networks, the training process consists of two loops, one for the training epochs and the other for the batches for each epoch. Inside the inner loop, you begin preparing the data to train the discriminator:"
      ],
      "metadata": {
        "id": "G-NmDHYxo57l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for n, (real_samples, _) in enumerate(train_loader):\n",
        "        # Data for training the discriminator\n",
        "        real_samples_labels = torch.ones((batch_size, 1))\n",
        "        latent_space_samples = torch.randn((batch_size, 28))\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        generated_samples_labels = torch.zeros((batch_size, 1))\n",
        "        all_samples = torch.cat((real_samples, generated_samples))\n",
        "        all_samples_labels = torch.cat(\n",
        "            (real_samples_labels, generated_samples_labels)\n",
        "        )\n",
        "\n",
        "        # Training the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        output_discriminator = discriminator(all_samples)\n",
        "        loss_discriminator = loss_function(\n",
        "            output_discriminator, all_samples_labels)\n",
        "        loss_discriminator.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        # Data for training the generator\n",
        "        latent_space_samples = torch.randn((batch_size, 28))\n",
        "\n",
        "        # Training the generator\n",
        "        generator.zero_grad()\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        output_discriminator_generated = discriminator(generated_samples)\n",
        "        loss_generator = loss_function(\n",
        "            output_discriminator_generated, real_samples_labels\n",
        "        )\n",
        "        loss_generator.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        # Show loss\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
        "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHuoLPnzpDcW",
        "outputId": "cab9ef6e-a698-4f96-8839-cffde998e89d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss D.: 0.381898432970047\n",
            "Epoch: 0 Loss G.: 2.7234854698181152\n",
            "Epoch: 0 Loss D.: 0.36823588609695435\n",
            "Epoch: 0 Loss G.: 1.9977753162384033\n",
            "Epoch: 0 Loss D.: 0.5384236574172974\n",
            "Epoch: 0 Loss G.: 2.5469017028808594\n",
            "Epoch: 10 Loss D.: 1.0200623273849487\n",
            "Epoch: 10 Loss G.: 0.49639958143234253\n",
            "Epoch: 10 Loss D.: 0.9482952356338501\n",
            "Epoch: 10 Loss G.: 0.5288295149803162\n",
            "Epoch: 10 Loss D.: 0.8713036179542542\n",
            "Epoch: 10 Loss G.: 0.5721114277839661\n",
            "Epoch: 20 Loss D.: 0.7942812442779541\n",
            "Epoch: 20 Loss G.: 1.169917106628418\n",
            "Epoch: 20 Loss D.: 0.7798004746437073\n",
            "Epoch: 20 Loss G.: 0.7656688690185547\n",
            "Epoch: 20 Loss D.: 0.783546507358551\n",
            "Epoch: 20 Loss G.: 0.8079279065132141\n",
            "Epoch: 30 Loss D.: 0.5640226006507874\n",
            "Epoch: 30 Loss G.: 2.3030900955200195\n",
            "Epoch: 30 Loss D.: 0.46974992752075195\n",
            "Epoch: 30 Loss G.: 2.425994634628296\n",
            "Epoch: 30 Loss D.: 0.6045116186141968\n",
            "Epoch: 30 Loss G.: 2.2504067420959473\n",
            "Epoch: 40 Loss D.: 0.43846380710601807\n",
            "Epoch: 40 Loss G.: 2.034878730773926\n",
            "Epoch: 40 Loss D.: 0.42095786333084106\n",
            "Epoch: 40 Loss G.: 2.5768918991088867\n",
            "Epoch: 40 Loss D.: 0.48551100492477417\n",
            "Epoch: 40 Loss G.: 2.6178104877471924\n",
            "Epoch: 50 Loss D.: 0.6286388635635376\n",
            "Epoch: 50 Loss G.: 0.6514169573783875\n",
            "Epoch: 50 Loss D.: 0.7015251517295837\n",
            "Epoch: 50 Loss G.: 0.6306496858596802\n",
            "Epoch: 50 Loss D.: 0.6535030603408813\n",
            "Epoch: 50 Loss G.: 0.755801260471344\n",
            "Epoch: 60 Loss D.: 0.4989345073699951\n",
            "Epoch: 60 Loss G.: 1.6280840635299683\n",
            "Epoch: 60 Loss D.: 0.44861337542533875\n",
            "Epoch: 60 Loss G.: 2.185950517654419\n",
            "Epoch: 60 Loss D.: 0.4729422628879547\n",
            "Epoch: 60 Loss G.: 3.5349836349487305\n",
            "Epoch: 70 Loss D.: 0.34581708908081055\n",
            "Epoch: 70 Loss G.: 2.8237967491149902\n",
            "Epoch: 70 Loss D.: 0.5604811906814575\n",
            "Epoch: 70 Loss G.: 1.5961651802062988\n",
            "Epoch: 70 Loss D.: 0.5012930631637573\n",
            "Epoch: 70 Loss G.: 1.3975999355316162\n",
            "Epoch: 80 Loss D.: 0.2450956553220749\n",
            "Epoch: 80 Loss G.: 6.3858323097229\n",
            "Epoch: 80 Loss D.: 0.22260931134223938\n",
            "Epoch: 80 Loss G.: 6.293605804443359\n",
            "Epoch: 80 Loss D.: 0.23371905088424683\n",
            "Epoch: 80 Loss G.: 6.304559707641602\n",
            "Epoch: 90 Loss D.: 0.4687785506248474\n",
            "Epoch: 90 Loss G.: 3.2688686847686768\n",
            "Epoch: 90 Loss D.: 0.3572377562522888\n",
            "Epoch: 90 Loss G.: 2.3318920135498047\n",
            "Epoch: 90 Loss D.: 0.44379255175590515\n",
            "Epoch: 90 Loss G.: 2.9383811950683594\n",
            "Epoch: 100 Loss D.: 0.8688921928405762\n",
            "Epoch: 100 Loss G.: 1.4583784341812134\n",
            "Epoch: 100 Loss D.: 0.8647312521934509\n",
            "Epoch: 100 Loss G.: 1.5977823734283447\n",
            "Epoch: 100 Loss D.: 1.126267433166504\n",
            "Epoch: 100 Loss G.: 1.6400086879730225\n",
            "Epoch: 110 Loss D.: 0.7638377547264099\n",
            "Epoch: 110 Loss G.: 0.8522420525550842\n",
            "Epoch: 110 Loss D.: 0.7560635805130005\n",
            "Epoch: 110 Loss G.: 1.1819149255752563\n",
            "Epoch: 110 Loss D.: 0.6541963815689087\n",
            "Epoch: 110 Loss G.: 1.1989091634750366\n",
            "Epoch: 120 Loss D.: 0.6119542717933655\n",
            "Epoch: 120 Loss G.: 1.071077585220337\n",
            "Epoch: 120 Loss D.: 0.5909243226051331\n",
            "Epoch: 120 Loss G.: 2.213202714920044\n",
            "Epoch: 120 Loss D.: 0.6018989086151123\n",
            "Epoch: 120 Loss G.: 2.2507741451263428\n",
            "Epoch: 130 Loss D.: 0.9834566712379456\n",
            "Epoch: 130 Loss G.: 0.3100988566875458\n",
            "Epoch: 130 Loss D.: 0.9417604207992554\n",
            "Epoch: 130 Loss G.: 0.30912870168685913\n",
            "Epoch: 130 Loss D.: 0.8760174512863159\n",
            "Epoch: 130 Loss G.: 0.3079752027988434\n",
            "Epoch: 140 Loss D.: 0.3780360817909241\n",
            "Epoch: 140 Loss G.: 4.067330360412598\n",
            "Epoch: 140 Loss D.: 0.4838342070579529\n",
            "Epoch: 140 Loss G.: 3.736802577972412\n",
            "Epoch: 140 Loss D.: 0.3662596642971039\n",
            "Epoch: 140 Loss G.: 3.9392571449279785\n",
            "Epoch: 150 Loss D.: 0.4776816964149475\n",
            "Epoch: 150 Loss G.: 3.083162307739258\n",
            "Epoch: 150 Loss D.: 0.4827856421470642\n",
            "Epoch: 150 Loss G.: 1.7218067646026611\n",
            "Epoch: 150 Loss D.: 0.6867389678955078\n",
            "Epoch: 150 Loss G.: 1.550663948059082\n",
            "Epoch: 160 Loss D.: 0.47284775972366333\n",
            "Epoch: 160 Loss G.: 3.1334991455078125\n",
            "Epoch: 160 Loss D.: 0.42205992341041565\n",
            "Epoch: 160 Loss G.: 2.813887596130371\n",
            "Epoch: 160 Loss D.: 0.4919016361236572\n",
            "Epoch: 160 Loss G.: 2.8701491355895996\n",
            "Epoch: 170 Loss D.: 0.3559667468070984\n",
            "Epoch: 170 Loss G.: 2.4008779525756836\n",
            "Epoch: 170 Loss D.: 0.39864349365234375\n",
            "Epoch: 170 Loss G.: 2.521998405456543\n",
            "Epoch: 170 Loss D.: 0.4172678589820862\n",
            "Epoch: 170 Loss G.: 2.8010342121124268\n",
            "Epoch: 180 Loss D.: 0.4494670629501343\n",
            "Epoch: 180 Loss G.: 4.0512800216674805\n",
            "Epoch: 180 Loss D.: 0.39347851276397705\n",
            "Epoch: 180 Loss G.: 2.778094530105591\n",
            "Epoch: 180 Loss D.: 0.7510133981704712\n",
            "Epoch: 180 Loss G.: 1.600219488143921\n",
            "Epoch: 190 Loss D.: 0.4397957921028137\n",
            "Epoch: 190 Loss G.: 3.76652193069458\n",
            "Epoch: 190 Loss D.: 0.4084397256374359\n",
            "Epoch: 190 Loss G.: 3.8732025623321533\n",
            "Epoch: 190 Loss D.: 0.5091816782951355\n",
            "Epoch: 190 Loss G.: 3.3347549438476562\n",
            "Epoch: 200 Loss D.: 0.43627071380615234\n",
            "Epoch: 200 Loss G.: 2.2559545040130615\n",
            "Epoch: 200 Loss D.: 0.5195199251174927\n",
            "Epoch: 200 Loss G.: 2.3506920337677\n",
            "Epoch: 200 Loss D.: 0.5846312046051025\n",
            "Epoch: 200 Loss G.: 2.137943744659424\n",
            "Epoch: 210 Loss D.: 0.455945760011673\n",
            "Epoch: 210 Loss G.: 2.8898019790649414\n",
            "Epoch: 210 Loss D.: 0.3163759112358093\n",
            "Epoch: 210 Loss G.: 2.807621479034424\n",
            "Epoch: 210 Loss D.: 0.4106372594833374\n",
            "Epoch: 210 Loss G.: 2.80161714553833\n",
            "Epoch: 220 Loss D.: 0.7600131034851074\n",
            "Epoch: 220 Loss G.: 1.0405926704406738\n",
            "Epoch: 220 Loss D.: 0.7442497611045837\n",
            "Epoch: 220 Loss G.: 0.8303436040878296\n",
            "Epoch: 220 Loss D.: 0.7305338382720947\n",
            "Epoch: 220 Loss G.: 0.8825593590736389\n",
            "Epoch: 230 Loss D.: 0.5183046460151672\n",
            "Epoch: 230 Loss G.: 3.6602602005004883\n",
            "Epoch: 230 Loss D.: 0.4878719449043274\n",
            "Epoch: 230 Loss G.: 2.8481504917144775\n",
            "Epoch: 230 Loss D.: 0.434084415435791\n",
            "Epoch: 230 Loss G.: 3.1608500480651855\n",
            "Epoch: 240 Loss D.: 0.3379678726196289\n",
            "Epoch: 240 Loss G.: 3.0135726928710938\n",
            "Epoch: 240 Loss D.: 0.2527909576892853\n",
            "Epoch: 240 Loss G.: 4.299882411956787\n",
            "Epoch: 240 Loss D.: 0.3895557224750519\n",
            "Epoch: 240 Loss G.: 3.7097625732421875\n",
            "Epoch: 250 Loss D.: 0.2830665409564972\n",
            "Epoch: 250 Loss G.: 4.214094638824463\n",
            "Epoch: 250 Loss D.: 0.31616875529289246\n",
            "Epoch: 250 Loss G.: 3.8949689865112305\n",
            "Epoch: 250 Loss D.: 0.27434587478637695\n",
            "Epoch: 250 Loss G.: 5.3258867263793945\n",
            "Epoch: 260 Loss D.: 0.37325167655944824\n",
            "Epoch: 260 Loss G.: 4.000356674194336\n",
            "Epoch: 260 Loss D.: 0.4402441084384918\n",
            "Epoch: 260 Loss G.: 2.858621597290039\n",
            "Epoch: 260 Loss D.: 0.6499357223510742\n",
            "Epoch: 260 Loss G.: 1.777081847190857\n",
            "Epoch: 270 Loss D.: 0.5823677778244019\n",
            "Epoch: 270 Loss G.: 4.152523994445801\n",
            "Epoch: 270 Loss D.: 0.5426332354545593\n",
            "Epoch: 270 Loss G.: 6.074864387512207\n",
            "Epoch: 270 Loss D.: 0.2415475994348526\n",
            "Epoch: 270 Loss G.: 7.2446513175964355\n",
            "Epoch: 280 Loss D.: 1.010622262954712\n",
            "Epoch: 280 Loss G.: 0.6449816226959229\n",
            "Epoch: 280 Loss D.: 0.74365234375\n",
            "Epoch: 280 Loss G.: 1.0364818572998047\n",
            "Epoch: 280 Loss D.: 0.620685338973999\n",
            "Epoch: 280 Loss G.: 2.6392335891723633\n",
            "Epoch: 290 Loss D.: 0.35953304171562195\n",
            "Epoch: 290 Loss G.: 2.6014106273651123\n",
            "Epoch: 290 Loss D.: 0.4232851564884186\n",
            "Epoch: 290 Loss G.: 2.41325044631958\n",
            "Epoch: 290 Loss D.: 0.4266539514064789\n",
            "Epoch: 290 Loss G.: 2.6648707389831543\n",
            "Epoch: 300 Loss D.: 0.49544695019721985\n",
            "Epoch: 300 Loss G.: 3.6839804649353027\n",
            "Epoch: 300 Loss D.: 0.4231066405773163\n",
            "Epoch: 300 Loss G.: 3.065667152404785\n",
            "Epoch: 300 Loss D.: 0.36838775873184204\n",
            "Epoch: 300 Loss G.: 3.4185314178466797\n",
            "Epoch: 310 Loss D.: 0.6902554035186768\n",
            "Epoch: 310 Loss G.: 1.2756223678588867\n",
            "Epoch: 310 Loss D.: 0.6574574708938599\n",
            "Epoch: 310 Loss G.: 1.368065357208252\n",
            "Epoch: 310 Loss D.: 0.5943657755851746\n",
            "Epoch: 310 Loss G.: 1.5216630697250366\n",
            "Epoch: 320 Loss D.: 0.6954200863838196\n",
            "Epoch: 320 Loss G.: 1.613638162612915\n",
            "Epoch: 320 Loss D.: 0.5202836990356445\n",
            "Epoch: 320 Loss G.: 2.239800214767456\n",
            "Epoch: 320 Loss D.: 0.45544859766960144\n",
            "Epoch: 320 Loss G.: 2.9355249404907227\n",
            "Epoch: 330 Loss D.: 0.6179718971252441\n",
            "Epoch: 330 Loss G.: 2.2953989505767822\n",
            "Epoch: 330 Loss D.: 0.591423511505127\n",
            "Epoch: 330 Loss G.: 2.5189216136932373\n",
            "Epoch: 330 Loss D.: 0.5066607594490051\n",
            "Epoch: 330 Loss G.: 2.719521999359131\n",
            "Epoch: 340 Loss D.: 0.35052692890167236\n",
            "Epoch: 340 Loss G.: 2.373967170715332\n",
            "Epoch: 340 Loss D.: 0.3243621587753296\n",
            "Epoch: 340 Loss G.: 3.3674049377441406\n",
            "Epoch: 340 Loss D.: 0.36295875906944275\n",
            "Epoch: 340 Loss G.: 2.361898899078369\n",
            "Epoch: 350 Loss D.: 0.535273551940918\n",
            "Epoch: 350 Loss G.: 1.6038854122161865\n",
            "Epoch: 350 Loss D.: 0.49992674589157104\n",
            "Epoch: 350 Loss G.: 4.002249717712402\n",
            "Epoch: 350 Loss D.: 0.4365765154361725\n",
            "Epoch: 350 Loss G.: 3.70503568649292\n",
            "Epoch: 360 Loss D.: 0.5211244821548462\n",
            "Epoch: 360 Loss G.: 1.7827951908111572\n",
            "Epoch: 360 Loss D.: 0.5711574554443359\n",
            "Epoch: 360 Loss G.: 1.6772677898406982\n",
            "Epoch: 360 Loss D.: 0.6279904246330261\n",
            "Epoch: 360 Loss G.: 2.35341739654541\n",
            "Epoch: 370 Loss D.: 0.8140923380851746\n",
            "Epoch: 370 Loss G.: 0.5935258865356445\n",
            "Epoch: 370 Loss D.: 0.7602792978286743\n",
            "Epoch: 370 Loss G.: 0.6950768232345581\n",
            "Epoch: 370 Loss D.: 0.7116005420684814\n",
            "Epoch: 370 Loss G.: 0.9138526320457458\n",
            "Epoch: 380 Loss D.: 0.7162759900093079\n",
            "Epoch: 380 Loss G.: 2.1012496948242188\n",
            "Epoch: 380 Loss D.: 0.6430648565292358\n",
            "Epoch: 380 Loss G.: 1.5729600191116333\n",
            "Epoch: 380 Loss D.: 0.507199764251709\n",
            "Epoch: 380 Loss G.: 2.0799505710601807\n",
            "Epoch: 390 Loss D.: 0.5795559287071228\n",
            "Epoch: 390 Loss G.: 3.1826977729797363\n",
            "Epoch: 390 Loss D.: 0.5111737847328186\n",
            "Epoch: 390 Loss G.: 3.0489983558654785\n",
            "Epoch: 390 Loss D.: 0.4127615988254547\n",
            "Epoch: 390 Loss G.: 3.226898431777954\n",
            "Epoch: 400 Loss D.: 0.667131781578064\n",
            "Epoch: 400 Loss G.: 0.8657882213592529\n",
            "Epoch: 400 Loss D.: 0.6356155276298523\n",
            "Epoch: 400 Loss G.: 1.430463433265686\n",
            "Epoch: 400 Loss D.: 0.5634939670562744\n",
            "Epoch: 400 Loss G.: 1.585458755493164\n",
            "Epoch: 410 Loss D.: 0.46189162135124207\n",
            "Epoch: 410 Loss G.: 3.3554141521453857\n",
            "Epoch: 410 Loss D.: 0.48169389367103577\n",
            "Epoch: 410 Loss G.: 3.1146647930145264\n",
            "Epoch: 410 Loss D.: 0.5300778150558472\n",
            "Epoch: 410 Loss G.: 3.0797057151794434\n",
            "Epoch: 420 Loss D.: 0.47856277227401733\n",
            "Epoch: 420 Loss G.: 2.629115581512451\n",
            "Epoch: 420 Loss D.: 0.44482436776161194\n",
            "Epoch: 420 Loss G.: 1.9945909976959229\n",
            "Epoch: 420 Loss D.: 0.49546170234680176\n",
            "Epoch: 420 Loss G.: 1.4400666952133179\n",
            "Epoch: 430 Loss D.: 0.43219515681266785\n",
            "Epoch: 430 Loss G.: 2.8528707027435303\n",
            "Epoch: 430 Loss D.: 0.39314529299736023\n",
            "Epoch: 430 Loss G.: 3.008220672607422\n",
            "Epoch: 430 Loss D.: 0.43382927775382996\n",
            "Epoch: 430 Loss G.: 2.9109344482421875\n",
            "Epoch: 440 Loss D.: 0.42176660895347595\n",
            "Epoch: 440 Loss G.: 3.1039047241210938\n",
            "Epoch: 440 Loss D.: 0.4804731011390686\n",
            "Epoch: 440 Loss G.: 3.9450736045837402\n",
            "Epoch: 440 Loss D.: 0.44886958599090576\n",
            "Epoch: 440 Loss G.: 3.077486991882324\n",
            "Epoch: 450 Loss D.: 0.6670086979866028\n",
            "Epoch: 450 Loss G.: 1.489959478378296\n",
            "Epoch: 450 Loss D.: 0.4064827561378479\n",
            "Epoch: 450 Loss G.: 2.192692995071411\n",
            "Epoch: 450 Loss D.: 0.4294399619102478\n",
            "Epoch: 450 Loss G.: 3.0805208683013916\n",
            "Epoch: 460 Loss D.: 0.30507737398147583\n",
            "Epoch: 460 Loss G.: 4.4550676345825195\n",
            "Epoch: 460 Loss D.: 0.38791102170944214\n",
            "Epoch: 460 Loss G.: 4.935352802276611\n",
            "Epoch: 460 Loss D.: 0.34911713004112244\n",
            "Epoch: 460 Loss G.: 6.00747013092041\n",
            "Epoch: 470 Loss D.: 0.42046454548835754\n",
            "Epoch: 470 Loss G.: 3.1150455474853516\n",
            "Epoch: 470 Loss D.: 0.3541407287120819\n",
            "Epoch: 470 Loss G.: 2.5326271057128906\n",
            "Epoch: 470 Loss D.: 0.4167100191116333\n",
            "Epoch: 470 Loss G.: 2.5770959854125977\n",
            "Epoch: 480 Loss D.: 0.3719278573989868\n",
            "Epoch: 480 Loss G.: 3.5852131843566895\n",
            "Epoch: 480 Loss D.: 0.4027213454246521\n",
            "Epoch: 480 Loss G.: 3.5318822860717773\n",
            "Epoch: 480 Loss D.: 0.4299864172935486\n",
            "Epoch: 480 Loss G.: 3.3313512802124023\n",
            "Epoch: 490 Loss D.: 0.5681023597717285\n",
            "Epoch: 490 Loss G.: 0.8340134024620056\n",
            "Epoch: 490 Loss D.: 0.8966494798660278\n",
            "Epoch: 490 Loss G.: 0.7323890328407288\n",
            "Epoch: 490 Loss D.: 0.7487331628799438\n",
            "Epoch: 490 Loss G.: 1.1076958179473877\n",
            "Epoch: 500 Loss D.: 0.5835561752319336\n",
            "Epoch: 500 Loss G.: 1.8376643657684326\n",
            "Epoch: 500 Loss D.: 0.5068085193634033\n",
            "Epoch: 500 Loss G.: 1.5437307357788086\n",
            "Epoch: 500 Loss D.: 0.5390819311141968\n",
            "Epoch: 500 Loss G.: 2.0505058765411377\n",
            "Epoch: 510 Loss D.: 0.43974146246910095\n",
            "Epoch: 510 Loss G.: 3.482966661453247\n",
            "Epoch: 510 Loss D.: 0.5941734313964844\n",
            "Epoch: 510 Loss G.: 2.2737412452697754\n",
            "Epoch: 510 Loss D.: 0.668480396270752\n",
            "Epoch: 510 Loss G.: 1.4475417137145996\n",
            "Epoch: 520 Loss D.: 0.5146560668945312\n",
            "Epoch: 520 Loss G.: 1.4932934045791626\n",
            "Epoch: 520 Loss D.: 0.5667769312858582\n",
            "Epoch: 520 Loss G.: 1.756124496459961\n",
            "Epoch: 520 Loss D.: 0.555642306804657\n",
            "Epoch: 520 Loss G.: 1.2038525342941284\n",
            "Epoch: 530 Loss D.: 0.5095923542976379\n",
            "Epoch: 530 Loss G.: 2.226081609725952\n",
            "Epoch: 530 Loss D.: 0.46576523780822754\n",
            "Epoch: 530 Loss G.: 1.7335875034332275\n",
            "Epoch: 530 Loss D.: 0.4982264041900635\n",
            "Epoch: 530 Loss G.: 1.3682262897491455\n",
            "Epoch: 540 Loss D.: 0.42747190594673157\n",
            "Epoch: 540 Loss G.: 3.143933057785034\n",
            "Epoch: 540 Loss D.: 0.3900161385536194\n",
            "Epoch: 540 Loss G.: 3.443399667739868\n",
            "Epoch: 540 Loss D.: 0.5292865037918091\n",
            "Epoch: 540 Loss G.: 3.0376296043395996\n",
            "Epoch: 550 Loss D.: 0.5707837343215942\n",
            "Epoch: 550 Loss G.: 2.072575807571411\n",
            "Epoch: 550 Loss D.: 0.491677850484848\n",
            "Epoch: 550 Loss G.: 2.1703834533691406\n",
            "Epoch: 550 Loss D.: 0.6258198022842407\n",
            "Epoch: 550 Loss G.: 2.040381908416748\n",
            "Epoch: 560 Loss D.: 0.5317286252975464\n",
            "Epoch: 560 Loss G.: 2.0848264694213867\n",
            "Epoch: 560 Loss D.: 0.514629602432251\n",
            "Epoch: 560 Loss G.: 1.643475890159607\n",
            "Epoch: 560 Loss D.: 0.6355615854263306\n",
            "Epoch: 560 Loss G.: 1.3455054759979248\n",
            "Epoch: 570 Loss D.: 0.5162091851234436\n",
            "Epoch: 570 Loss G.: 1.6420634984970093\n",
            "Epoch: 570 Loss D.: 0.5383808612823486\n",
            "Epoch: 570 Loss G.: 2.1048617362976074\n",
            "Epoch: 570 Loss D.: 0.39335259795188904\n",
            "Epoch: 570 Loss G.: 2.685948371887207\n",
            "Epoch: 580 Loss D.: 0.39718854427337646\n",
            "Epoch: 580 Loss G.: 4.625386714935303\n",
            "Epoch: 580 Loss D.: 0.3773764371871948\n",
            "Epoch: 580 Loss G.: 3.4305269718170166\n",
            "Epoch: 580 Loss D.: 0.37344086170196533\n",
            "Epoch: 580 Loss G.: 3.6329708099365234\n",
            "Epoch: 590 Loss D.: 0.5565646290779114\n",
            "Epoch: 590 Loss G.: 1.4165079593658447\n",
            "Epoch: 590 Loss D.: 0.45360684394836426\n",
            "Epoch: 590 Loss G.: 1.9785598516464233\n",
            "Epoch: 590 Loss D.: 0.4125247299671173\n",
            "Epoch: 590 Loss G.: 1.7248786687850952\n",
            "Epoch: 600 Loss D.: 0.35856449604034424\n",
            "Epoch: 600 Loss G.: 3.0338683128356934\n",
            "Epoch: 600 Loss D.: 0.2860107719898224\n",
            "Epoch: 600 Loss G.: 2.8121840953826904\n",
            "Epoch: 600 Loss D.: 0.2647324502468109\n",
            "Epoch: 600 Loss G.: 4.085036754608154\n",
            "Epoch: 610 Loss D.: 0.39530783891677856\n",
            "Epoch: 610 Loss G.: 2.7919816970825195\n",
            "Epoch: 610 Loss D.: 0.33977675437927246\n",
            "Epoch: 610 Loss G.: 2.342790365219116\n",
            "Epoch: 610 Loss D.: 0.3425033688545227\n",
            "Epoch: 610 Loss G.: 2.7235071659088135\n",
            "Epoch: 620 Loss D.: 0.40356913208961487\n",
            "Epoch: 620 Loss G.: 3.1062920093536377\n",
            "Epoch: 620 Loss D.: 0.3949929475784302\n",
            "Epoch: 620 Loss G.: 4.11416482925415\n",
            "Epoch: 620 Loss D.: 0.32848384976387024\n",
            "Epoch: 620 Loss G.: 3.481015682220459\n",
            "Epoch: 630 Loss D.: 0.3720962405204773\n",
            "Epoch: 630 Loss G.: 4.492967128753662\n",
            "Epoch: 630 Loss D.: 0.3259645402431488\n",
            "Epoch: 630 Loss G.: 4.416172981262207\n",
            "Epoch: 630 Loss D.: 0.4274859130382538\n",
            "Epoch: 630 Loss G.: 4.001544952392578\n",
            "Epoch: 640 Loss D.: 0.442971408367157\n",
            "Epoch: 640 Loss G.: 2.765700578689575\n",
            "Epoch: 640 Loss D.: 0.33250540494918823\n",
            "Epoch: 640 Loss G.: 2.7885894775390625\n",
            "Epoch: 640 Loss D.: 0.31731897592544556\n",
            "Epoch: 640 Loss G.: 3.5191287994384766\n",
            "Epoch: 650 Loss D.: 0.9171358346939087\n",
            "Epoch: 650 Loss G.: 0.6176662445068359\n",
            "Epoch: 650 Loss D.: 0.8800898790359497\n",
            "Epoch: 650 Loss G.: 0.5234661102294922\n",
            "Epoch: 650 Loss D.: 0.8854672312736511\n",
            "Epoch: 650 Loss G.: 0.5385838150978088\n",
            "Epoch: 660 Loss D.: 0.49707528948783875\n",
            "Epoch: 660 Loss G.: 6.6022138595581055\n",
            "Epoch: 660 Loss D.: 0.3474140167236328\n",
            "Epoch: 660 Loss G.: 5.066572666168213\n",
            "Epoch: 660 Loss D.: 0.33881592750549316\n",
            "Epoch: 660 Loss G.: 4.539730072021484\n",
            "Epoch: 670 Loss D.: 0.5796375870704651\n",
            "Epoch: 670 Loss G.: 3.5367484092712402\n",
            "Epoch: 670 Loss D.: 0.6104069352149963\n",
            "Epoch: 670 Loss G.: 3.2532243728637695\n",
            "Epoch: 670 Loss D.: 0.7696382403373718\n",
            "Epoch: 670 Loss G.: 2.8925623893737793\n",
            "Epoch: 680 Loss D.: 0.7251678109169006\n",
            "Epoch: 680 Loss G.: 0.8611619472503662\n",
            "Epoch: 680 Loss D.: 0.6513562798500061\n",
            "Epoch: 680 Loss G.: 1.1063189506530762\n",
            "Epoch: 680 Loss D.: 0.5540640950202942\n",
            "Epoch: 680 Loss G.: 1.3840941190719604\n",
            "Epoch: 690 Loss D.: 0.41868188977241516\n",
            "Epoch: 690 Loss G.: 3.5517449378967285\n",
            "Epoch: 690 Loss D.: 0.40700763463974\n",
            "Epoch: 690 Loss G.: 2.7627065181732178\n",
            "Epoch: 690 Loss D.: 0.46286872029304504\n",
            "Epoch: 690 Loss G.: 4.38668966293335\n",
            "Epoch: 700 Loss D.: 0.600699782371521\n",
            "Epoch: 700 Loss G.: 2.310823917388916\n",
            "Epoch: 700 Loss D.: 0.5328788161277771\n",
            "Epoch: 700 Loss G.: 2.5107884407043457\n",
            "Epoch: 700 Loss D.: 0.6580827832221985\n",
            "Epoch: 700 Loss G.: 2.221409559249878\n",
            "Epoch: 710 Loss D.: 0.7312107682228088\n",
            "Epoch: 710 Loss G.: 1.1724716424942017\n",
            "Epoch: 710 Loss D.: 0.756393551826477\n",
            "Epoch: 710 Loss G.: 1.019657015800476\n",
            "Epoch: 710 Loss D.: 0.5704429149627686\n",
            "Epoch: 710 Loss G.: 0.9510339498519897\n",
            "Epoch: 720 Loss D.: 0.2864619493484497\n",
            "Epoch: 720 Loss G.: 3.7859201431274414\n",
            "Epoch: 720 Loss D.: 0.4346734881401062\n",
            "Epoch: 720 Loss G.: 2.9754743576049805\n",
            "Epoch: 720 Loss D.: 0.33197733759880066\n",
            "Epoch: 720 Loss G.: 2.278510808944702\n",
            "Epoch: 730 Loss D.: 0.39627885818481445\n",
            "Epoch: 730 Loss G.: 3.0907294750213623\n",
            "Epoch: 730 Loss D.: 0.43903160095214844\n",
            "Epoch: 730 Loss G.: 1.9318506717681885\n",
            "Epoch: 730 Loss D.: 0.5454427003860474\n",
            "Epoch: 730 Loss G.: 1.877962350845337\n",
            "Epoch: 740 Loss D.: 0.3392447531223297\n",
            "Epoch: 740 Loss G.: 2.768101453781128\n",
            "Epoch: 740 Loss D.: 0.37692952156066895\n",
            "Epoch: 740 Loss G.: 2.1504428386688232\n",
            "Epoch: 740 Loss D.: 0.4092825949192047\n",
            "Epoch: 740 Loss G.: 3.0753188133239746\n",
            "Epoch: 750 Loss D.: 0.47422388195991516\n",
            "Epoch: 750 Loss G.: 4.136221885681152\n",
            "Epoch: 750 Loss D.: 0.22273129224777222\n",
            "Epoch: 750 Loss G.: 3.6088523864746094\n",
            "Epoch: 750 Loss D.: 0.37732023000717163\n",
            "Epoch: 750 Loss G.: 3.1078591346740723\n",
            "Epoch: 760 Loss D.: 0.3956460952758789\n",
            "Epoch: 760 Loss G.: 2.943403482437134\n",
            "Epoch: 760 Loss D.: 0.4856877624988556\n",
            "Epoch: 760 Loss G.: 3.906087875366211\n",
            "Epoch: 760 Loss D.: 0.7146244645118713\n",
            "Epoch: 760 Loss G.: 3.7993359565734863\n",
            "Epoch: 770 Loss D.: 0.4486438035964966\n",
            "Epoch: 770 Loss G.: 2.5209789276123047\n",
            "Epoch: 770 Loss D.: 0.557870626449585\n",
            "Epoch: 770 Loss G.: 2.9829792976379395\n",
            "Epoch: 770 Loss D.: 0.4446820914745331\n",
            "Epoch: 770 Loss G.: 3.318513870239258\n",
            "Epoch: 780 Loss D.: 0.33475494384765625\n",
            "Epoch: 780 Loss G.: 6.324027061462402\n",
            "Epoch: 780 Loss D.: 0.38504236936569214\n",
            "Epoch: 780 Loss G.: 6.15551233291626\n",
            "Epoch: 780 Loss D.: 0.3893378973007202\n",
            "Epoch: 780 Loss G.: 5.933337688446045\n",
            "Epoch: 790 Loss D.: 0.40020620822906494\n",
            "Epoch: 790 Loss G.: 4.662642478942871\n",
            "Epoch: 790 Loss D.: 0.4009094834327698\n",
            "Epoch: 790 Loss G.: 4.363450050354004\n",
            "Epoch: 790 Loss D.: 0.5798150300979614\n",
            "Epoch: 790 Loss G.: 4.093513488769531\n",
            "Epoch: 800 Loss D.: 0.6239323616027832\n",
            "Epoch: 800 Loss G.: 1.7078534364700317\n",
            "Epoch: 800 Loss D.: 0.578648567199707\n",
            "Epoch: 800 Loss G.: 2.168112277984619\n",
            "Epoch: 800 Loss D.: 0.5742624998092651\n",
            "Epoch: 800 Loss G.: 3.117776393890381\n",
            "Epoch: 810 Loss D.: 0.49095895886421204\n",
            "Epoch: 810 Loss G.: 3.8639118671417236\n",
            "Epoch: 810 Loss D.: 0.5285401344299316\n",
            "Epoch: 810 Loss G.: 3.9688165187835693\n",
            "Epoch: 810 Loss D.: 0.4887792468070984\n",
            "Epoch: 810 Loss G.: 2.5284883975982666\n",
            "Epoch: 820 Loss D.: 0.2451423853635788\n",
            "Epoch: 820 Loss G.: 5.108391761779785\n",
            "Epoch: 820 Loss D.: 0.2569597363471985\n",
            "Epoch: 820 Loss G.: 3.466599941253662\n",
            "Epoch: 820 Loss D.: 0.38473770022392273\n",
            "Epoch: 820 Loss G.: 2.783428907394409\n",
            "Epoch: 830 Loss D.: 0.43979430198669434\n",
            "Epoch: 830 Loss G.: 2.959214925765991\n",
            "Epoch: 830 Loss D.: 0.47241324186325073\n",
            "Epoch: 830 Loss G.: 3.9904866218566895\n",
            "Epoch: 830 Loss D.: 0.24877503514289856\n",
            "Epoch: 830 Loss G.: 4.531208038330078\n",
            "Epoch: 840 Loss D.: 0.28783151507377625\n",
            "Epoch: 840 Loss G.: 4.91187858581543\n",
            "Epoch: 840 Loss D.: 0.2239028662443161\n",
            "Epoch: 840 Loss G.: 4.849445819854736\n",
            "Epoch: 840 Loss D.: 0.2346855103969574\n",
            "Epoch: 840 Loss G.: 5.452119827270508\n",
            "Epoch: 850 Loss D.: 0.8375474214553833\n",
            "Epoch: 850 Loss G.: 0.5316003561019897\n",
            "Epoch: 850 Loss D.: 1.0226365327835083\n",
            "Epoch: 850 Loss G.: 0.489786833524704\n",
            "Epoch: 850 Loss D.: 1.076542854309082\n",
            "Epoch: 850 Loss G.: 0.374923974275589\n",
            "Epoch: 860 Loss D.: 0.45445987582206726\n",
            "Epoch: 860 Loss G.: 2.6499764919281006\n",
            "Epoch: 860 Loss D.: 0.5149005651473999\n",
            "Epoch: 860 Loss G.: 3.026876211166382\n",
            "Epoch: 860 Loss D.: 0.5188442468643188\n",
            "Epoch: 860 Loss G.: 3.3314402103424072\n",
            "Epoch: 870 Loss D.: 0.6433953046798706\n",
            "Epoch: 870 Loss G.: 2.486405372619629\n",
            "Epoch: 870 Loss D.: 0.42549315094947815\n",
            "Epoch: 870 Loss G.: 3.5351226329803467\n",
            "Epoch: 870 Loss D.: 0.3450508415699005\n",
            "Epoch: 870 Loss G.: 4.277290344238281\n",
            "Epoch: 880 Loss D.: 0.4997386932373047\n",
            "Epoch: 880 Loss G.: 4.218502998352051\n",
            "Epoch: 880 Loss D.: 0.5469549894332886\n",
            "Epoch: 880 Loss G.: 3.0843052864074707\n",
            "Epoch: 880 Loss D.: 0.6201345324516296\n",
            "Epoch: 880 Loss G.: 2.153034210205078\n",
            "Epoch: 890 Loss D.: 0.5597753524780273\n",
            "Epoch: 890 Loss G.: 1.5805538892745972\n",
            "Epoch: 890 Loss D.: 0.5260066390037537\n",
            "Epoch: 890 Loss G.: 1.6082762479782104\n",
            "Epoch: 890 Loss D.: 0.4986153542995453\n",
            "Epoch: 890 Loss G.: 1.5248347520828247\n",
            "Epoch: 900 Loss D.: 0.357888787984848\n",
            "Epoch: 900 Loss G.: 2.6409411430358887\n",
            "Epoch: 900 Loss D.: 0.43495863676071167\n",
            "Epoch: 900 Loss G.: 2.2245075702667236\n",
            "Epoch: 900 Loss D.: 0.45619386434555054\n",
            "Epoch: 900 Loss G.: 1.934267282485962\n",
            "Epoch: 910 Loss D.: 0.5232370495796204\n",
            "Epoch: 910 Loss G.: 3.7428293228149414\n",
            "Epoch: 910 Loss D.: 0.445212721824646\n",
            "Epoch: 910 Loss G.: 3.698554515838623\n",
            "Epoch: 910 Loss D.: 0.41091397404670715\n",
            "Epoch: 910 Loss G.: 3.093378782272339\n",
            "Epoch: 920 Loss D.: 0.23428022861480713\n",
            "Epoch: 920 Loss G.: 4.689131259918213\n",
            "Epoch: 920 Loss D.: 0.3011108934879303\n",
            "Epoch: 920 Loss G.: 4.041421890258789\n",
            "Epoch: 920 Loss D.: 0.4528399109840393\n",
            "Epoch: 920 Loss G.: 3.1904823780059814\n",
            "Epoch: 930 Loss D.: 0.676163375377655\n",
            "Epoch: 930 Loss G.: 2.952965021133423\n",
            "Epoch: 930 Loss D.: 0.5345524549484253\n",
            "Epoch: 930 Loss G.: 2.4795985221862793\n",
            "Epoch: 930 Loss D.: 0.5887094140052795\n",
            "Epoch: 930 Loss G.: 1.5436737537384033\n",
            "Epoch: 940 Loss D.: 0.3495127856731415\n",
            "Epoch: 940 Loss G.: 3.6362617015838623\n",
            "Epoch: 940 Loss D.: 0.5421215891838074\n",
            "Epoch: 940 Loss G.: 2.1830008029937744\n",
            "Epoch: 940 Loss D.: 0.4633559584617615\n",
            "Epoch: 940 Loss G.: 3.1411876678466797\n",
            "Epoch: 950 Loss D.: 0.41646721959114075\n",
            "Epoch: 950 Loss G.: 2.8530871868133545\n",
            "Epoch: 950 Loss D.: 0.37671661376953125\n",
            "Epoch: 950 Loss G.: 2.541069984436035\n",
            "Epoch: 950 Loss D.: 0.4436533451080322\n",
            "Epoch: 950 Loss G.: 4.215357303619385\n",
            "Epoch: 960 Loss D.: 0.4857218265533447\n",
            "Epoch: 960 Loss G.: 2.3331940174102783\n",
            "Epoch: 960 Loss D.: 0.5577767491340637\n",
            "Epoch: 960 Loss G.: 2.1266088485717773\n",
            "Epoch: 960 Loss D.: 0.5640951991081238\n",
            "Epoch: 960 Loss G.: 1.772497534751892\n",
            "Epoch: 970 Loss D.: 0.41335105895996094\n",
            "Epoch: 970 Loss G.: 3.4171016216278076\n",
            "Epoch: 970 Loss D.: 0.448897123336792\n",
            "Epoch: 970 Loss G.: 2.5341334342956543\n",
            "Epoch: 970 Loss D.: 0.5127163529396057\n",
            "Epoch: 970 Loss G.: 3.4860827922821045\n",
            "Epoch: 980 Loss D.: 0.45940539240837097\n",
            "Epoch: 980 Loss G.: 3.9371695518493652\n",
            "Epoch: 980 Loss D.: 0.41664499044418335\n",
            "Epoch: 980 Loss G.: 3.5975141525268555\n",
            "Epoch: 980 Loss D.: 0.3616451323032379\n",
            "Epoch: 980 Loss G.: 2.694394111633301\n",
            "Epoch: 990 Loss D.: 0.724595844745636\n",
            "Epoch: 990 Loss G.: 0.4790918827056885\n",
            "Epoch: 990 Loss D.: 0.7125144600868225\n",
            "Epoch: 990 Loss G.: 0.4991505742073059\n",
            "Epoch: 990 Loss D.: 0.730319619178772\n",
            "Epoch: 990 Loss G.: 0.5184012055397034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the Samples Generated by the GAN\n",
        "Generative adversarial networks are designed to generate data. So, after the training process is finished, you can get some random samples from the latent space and feed them to the generator to obtain some generated samples:"
      ],
      "metadata": {
        "id": "weqsOst-p4iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_space_samples = torch.randn(10, 28)\n",
        "generated_samples = generator(latent_space_samples)"
      ],
      "metadata": {
        "id": "2LPETPYgp3tS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_samples"
      ],
      "metadata": {
        "id": "Ow0HQVyek1a-",
        "outputId": "e02aca85-5dad-43cb-ab17-6229a818d7d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065],\n",
              "        [ 0.0872,  0.1249,  0.0902,  0.0573,  0.0697,  0.0924,  0.0745,  0.0464,\n",
              "          0.0754,  0.0754,  0.0691,  0.0487,  0.0746,  0.0294, -0.0083, -0.0082,\n",
              "         -0.0021, -0.0027,  0.0099, -0.0116,  0.0052,  0.0002, -0.0056, -0.0111,\n",
              "         -0.0122,  0.0077,  0.0040, -0.0065]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_samples = generated_samples.detach()\n",
        "plt.plot(generated_samples[0,0:14], generated_samples[0,14:28], \"o-\")"
      ],
      "metadata": {
        "id": "1rBq11ULxTbC",
        "outputId": "15e52d51-310b-4fe3-ba47-69a535e97ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8b8cbe4050>]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_samples = generated_samples.detach()\n",
        "plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")"
      ],
      "metadata": {
        "id": "8lXjD9ogqNxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(generated_samples[:, 0], generated_samples[:, 2], \".\")"
      ],
      "metadata": {
        "id": "mPXCmCaKbwqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(generated_samples[:, 1], generated_samples[:, 2], \"-o\")"
      ],
      "metadata": {
        "id": "qWxYHcd5j-lf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}