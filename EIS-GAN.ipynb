{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN for EIS data simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It’s a good practice to set up a random generator seed so that the experiment can be replicated identically on any machine. \n",
    "#To do that in PyTorch:\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/'\n",
    "sig_i_10 = pd.read_csv(DATA_PATH+'batt2_meas3_sig_i_10.csv')\n",
    "sig_v_10 = pd.read_csv(DATA_PATH+'batt2_meas3_sig_v_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(sig_v_10.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_length = len(sig_i_10)\n",
    "# initialize train_data, a tensor with dimensions of <train_data_length> rows and 2 columns, all containing zeros. \n",
    "# Note: A tensor is a multidimensional array similar to a NumPy array.\n",
    "train_data = torch.zeros((train_data_length, 2))\n",
    "\n",
    "#for i  in range(0,train_data_length):\n",
    "#  f_index = i % 14\n",
    "#  # the first column of train_data store the frequency values\n",
    "#  train_data[i, 0] = frequency_list[f_index]\n",
    "\n",
    "train_data[:, 0] = torch.tensor(range(0, train_data_length))\n",
    "\n",
    "# calculate the second column of the tensor as the sine of the first column.\n",
    "train_data[:, 1] = torch.from_numpy(sig_v_10.values.transpose())\n",
    "\n",
    "\n",
    "# init tensor of labels, which are required by PyTorch’s data loader. \n",
    "# Since GANs make use of unsupervised learning techniques, the labels can be anything.\n",
    "train_labels = torch.zeros(train_data_length)\n",
    "\n",
    "# create train_set as a list of tuples, with each row of train_data and train_labels represented in each tuple \n",
    "# this is the format expected by PyTorch’s data loader.\n",
    "train_set = [\n",
    "    (train_data[i], train_labels[i]) for i in range(train_data_length)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sig_i_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This discriminator is an MLP neural network \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            #The input is two-dimensional, and the first hidden layer is composed \n",
    "            # of 256 neurons with ReLU activation.\n",
    "            nn.Linear(2, 256),\n",
    "            nn.ReLU(),\n",
    "            # use dropout after hidden layer to avoid overfitting.\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            # The output is composed of a single neuron with sigmoidal activation\n",
    "            # to represent a probability.\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "     # .forward() to describe how the output of the model is calculated\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "# instantiate a Discriminator object\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch data loader called train_loader.\n",
    "# Data Loader will shuffle the data from train_set and return batches of <batch_size> samples \n",
    "# to be used to train the neural networks.\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, drop_last=True, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# The binary cross-entropy function is a suitable loss function for training the discriminator \n",
    "# because it considers a binary classification task. \n",
    "# It’s also suitable for training the generator since it feeds its output to the discriminator,\n",
    "# which provides a binary observable output.\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for n, (real_samples, _) in enumerate(train_loader):\n",
    "        # Data for training the discriminator\n",
    "        real_samples_labels = torch.ones((batch_size, 1))\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        generated_samples_labels = torch.zeros((batch_size, 1))\n",
    "        all_samples = torch.cat((real_samples, generated_samples))\n",
    "        all_samples_labels = torch.cat(\n",
    "            (real_samples_labels, generated_samples_labels)\n",
    "        )\n",
    "\n",
    "        # Training the discriminator\n",
    "        discriminator.zero_grad()\n",
    "        output_discriminator = discriminator(all_samples)\n",
    "        loss_discriminator = loss_function(\n",
    "            output_discriminator, all_samples_labels)\n",
    "        loss_discriminator.backward()\n",
    "        \n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        # Data for training the generator\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "\n",
    "        # Training the generator\n",
    "        generator.zero_grad()\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        output_discriminator_generated = discriminator(generated_samples)\n",
    "        loss_generator = loss_function(\n",
    "            output_discriminator_generated, real_samples_labels\n",
    "        )\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Show loss\n",
    "        if epoch % 10 == 0 and n == batch_size - 1:\n",
    "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_samples = torch.randn(1000, 3)\n",
    "generated_samples = generator(latent_space_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10ac610c3e92572f9a22face5588ea509fa9f41d041b79c065588094f713a205"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
